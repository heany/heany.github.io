[{"title":"golang基础学习","date":"2019-02-02T11:40:55.000Z","path":"2019/02/02/golang基础学习/","text":"一、 前言Go编程语言是一个开源项目，它使程序员更具生产力。Go语言具有很强的表达能力，它简洁、清晰而高效。得益于其并发机制，用它编写的程序能够非常有效地利用多核与联网的计算机，其新颖的类型系统则使程序结构变得灵活而模块化。Go代码编译成机器码不仅非常迅速，还具有方便的垃圾收集机制和强大的运行时反射机制，它是一个快速的、静态类型的编译型语言，感觉却像动态类型的解释型语言。 第一个go程序 1234567package mainimport ( \"fmt\")func main() &#123; fmt.Println(\"hello world!!!gogogogo\")&#125; 解释：如果是为了将代码编译成一个可执行程序，那么package必须时main如果是为了将代码编译成库，那么pacakge则没有限制Go中所有的代码都应该隶属一个包。 fmt是go的一个系统库fmt.Println()则可以打印输出，如果想要运行程序，执行命令go run 程序名 在一个可执行程序中只能只有一个main函数。 go的结构开发规范 好的代码规范非常重要，这样当你看别人的代码或者别人看你的代码的时候就能很清楚的明白，下面是go程序基本的结构规范：123456789101112131415161718//当前程序的包名package main//导入其他的包import \"fmt\"//常量的定义const PI =3.14//全局变量的声明和赋值var name = \"gopher\"//一般类型声明type newType int//结构的声明type gopher struct&#123;&#125;//接口的声明type golang interface&#123;&#125;//函数func funcName(a type1, b type2) (c type1, d type2) &#123; return c, d&#125; 二、基础语法2.1 变量函数内声明的为局部变量，作用域为函数体，函数外声明的变量为全局变量，作用域为包。12345678910111213// 声明：var a int // 声明 int 类型的变量var b [10] int // 声明 int 类型数组var c []int // 声明 int 类型的切片var d *int // 声明 int 类型的指针// 赋值：a = 10b[0] = 10// 同时声明与赋值var a = 10//等价于a := 10a,b,c,d := 1,2,true,\"goodboy\" 2.2 常量123456789101112const filename = \"ab\"const a,b = 3,4 //常量可作为各种类型调用，const( java = 1 php = 2 python = 3)const( java = iota // 自增值，初始为0 php python) 2.3 条件语句与循环if1234567891011if a &gt; 100 &#123; return 100&#125;else if a &gt;50 &#123; return 50&#125;else&#123; return 0&#125;if a,b := 1,2; a+b&gt;3&#123; fmt.Println(a,b)&#125;fmt.Println(a,b) // 错误！ a,b的是 if 条件里定义的，作用域仅限于 if 中使用 利用if语句判断读取文件时是否有异常 12345678910111213import ( \"io/ioutil\" \"fmt\")func main() &#123; const filename = \"2.txt\" content, err := ioutil.ReadFile(filename) if(err!=nil)&#123; fmt.Println(err) &#125;else &#123; fmt.Printf(\"%s\", content) &#125;&#125; switch go中的switch不需要手动break1234567891011121314151617181920var grade string = \"B\"switch marks &#123; case 90: grade = \"A\" case 80: grade = \"B\" case 50,60,70 : grade = \"C\" default: grade = \"D\" &#125;switch &#123; case grade == \"A\" : fmt.Printf(\"优秀!\\n\" ) case grade == \"B\", grade == \"C\" : fmt.Printf(\"良好\\n\" ) case grade == \"D\" : fmt.Printf(\"及格\\n\" ) case grade == \"F\": fmt.Printf(\"不及格\\n\" ) default: fmt.Printf(\"差\\n\" );&#125;fmt.Printf(\"你的等级是 %s\\n\", grade ); for12345678910111213141516171819// 赋值语句；判断语句；递增语句for i:=100; i&gt;0; i--&#123; fmt.Println(i)&#125;// 无赋值func test(n int)&#123; for ; n&gt;0 ; n/=2 &#123; fmt.Println(n); &#125;&#125;// 仅赋值scanner := bufio.NewScanner(file)for scanner.Scan()&#123; fmt.Println(scanner.Text);&#125;// 死循环for&#123; fmt.Println(1);&#125; for range语句12345str := \"hello 世界\"for i,v := range str &#123; fmt.Println(\"index[%d] val[%c] len[%d]\\n\",i,v,len([]byte(string(v))))&#125;//这里需要注意一个问题，range str 返回的是两个值，一个是字符串的下标，一个是字符串中单个字符 label1234567891011121314package mainimport \"fmt\"func main() &#123; LABEL1:for i:=0;i&lt;5;i++&#123; for j:=0;j&lt;5;j++&#123; if j == 4&#123; continue LABEL1 &#125; fmt.Printf(\"i is :%d and j is:%d\\n\",i,j) &#125; &#125;&#125; 代码中我们在continue 后面添加了一个LABEL1这样当循环匹配到j等于4的时候，就会跳出循环，重新回到最外成i的循环，而如果没有LABEL1则就会跳出j的本次循环，执行j++进入到j的下次循环 2.4 函数2.4.1 函数声明1234567//语法：func 函数名(参数列表)（返回列表）&#123;&#125;//一些实例func add()&#123;&#125;func add(a int,b int)&#123;&#125;func add(a int,b int) int &#123;&#125;func add(a int,b int) (int,int) &#123;&#125;func add(a,b int)(int,int)&#123;&#125; 2.4.2 go函数的特点 不支持重载，一个包不能包含两个名字一样的函数 函数是一等公民，函数也是一种类型，一个函数可以赋值给变量 匿名函数 多返回值 下面通过例子来演示第二个特点123456789101112131415161718192021package mainimport ( \"fmt\")type op_func func(int,int) intfunc add(a,b int) int &#123; return a + b&#125;func operator(op op_func,a,b int) int&#123; return op(a,b)&#125;func main() &#123; c := add sum := operator(c,100,200) fmt.Println(sum)&#125; 2.4.3 可变参数表示0个或多个参数1func add(arg ...int) int &#123;&#125; 表示1个或多个参数 func add(a int,arg ...int) int {} 其中arg是一个slice，我们可以通过arg[index]获取参数，通过len(arg)可以判断参数的个数 2.5 指针go语言的参数传递是值传递,无论是值传递还是引用传递，传递给函数的都是变量的副本，不过值传递的是值的拷贝，引用传递是传递的地址的拷贝，一般来说，地址拷贝更为高效，而值拷贝取决于拷贝的对象的大小，对象越大，则性能越低 12345678func main() &#123; a,b:=1,2 swap_test(&amp;a,&amp;b)&#125;func swap_test(a,b *int) &#123; fmt.Println(a, *b) // 0xc420014050 2&#125;// 理解： a,b *int 存的是 int 类型值的地址，当对指针类型的变量 *a 时，就是取出地址对应的值 普通的类型，变量存的就是值，也叫值类型 获取变量的地址，用&amp;， 指针类型，变量存的是一个地址，这个地址存的才是真正的值 获取指针类型所指向的值，用,例如：var p int, 使用 *p获取p指向值 通过下面的代码例子理解：1234567891011121314151617package mainimport \"fmt\"func main() &#123; var a int = 10 //通过&amp;a打印a的指针地址 fmt.Println(&amp;a) //定义一个指针类型的变量p var p *int //讲a的指针地址复制给p p = &amp;a fmt.Println(*p) //给指针p赋值 *p = 100 fmt.Println(a)&#125; 三、内建容器3.1 数组3.2 切片3.3 Map四、面向对象4.1 结构体4.2 封装4.3 接口","tags":[{"name":"golang","slug":"golang","permalink":"http://heany.github.io/tags/golang/"}]},{"title":"Redis学习笔记","date":"2019-02-02T11:33:23.000Z","path":"2019/02/02/Redis学习笔记/","text":"一、 前言redis学习笔记，先占坑，年后在写","tags":[{"name":"redis","slug":"redis","permalink":"http://heany.github.io/tags/redis/"}]},{"title":"mysql操作使用变量作为表名","date":"2018-06-08T02:42:11.000Z","path":"2018/06/08/mysql操作使用变量作为表名/","text":"版权声明：本文为博主原创文章，未经博主允许不得转载。 使用python操作mysql数据库时想实现如下效果：将sql语句中的表名用一个变量代替，实现批量操作表 sql = &quot;select * from sets where number = &apos;%s&apos; &quot; 将sql语句中的sets和number用变量代替，实现自动操作多个表： 1234table = \"sets\"attr = \"number\"sql = \"select * from \"+table+\" where \"+attr+\" = '%s'\"# 成功运行，达到要求 其实就是采用字符串操作中的语句拼接技术","tags":[]},{"title":"Numpy的random函数汇总","date":"2018-06-06T01:54:44.000Z","path":"2018/06/06/Numpy的random函数汇总/","text":"一、前言在python数据分析的学习和日常应用过程中，经常要用到numpy库的随机函数，这些随机函数功能很多，经常会混淆记不住～～，每次使用都要去Google，所以我就把常用的随机函数功能汇总在下面了，方便自己查阅。 1import numpy as np 二、numpy.random.rand()语法：1234numpy.random.rand(d0,d1,...,dn)# rand函数根据给定维度生成[0,1)之间的数据，包含0，不包含1# dn表格每个维度# 返回值为指定维度的array 例子：123456789101112131415161718192021222324np.random.rand(4,2)# outputarray([[0.44966653 0.52431591] [0.12117809 0.31425446] [0.94938082 0.78223903] [0.98802125 0.01795372]])np.random.rand(4,3,2)#outputarray([[[0.4016697 0.41116077] [0.81121991 0.18433693] [0.94307831 0.37959419]] [[0.11907587 0.97932363] [0.20458545 0.73305606] [0.62938875 0.99506056]] [[0.85168379 0.76133277] [0.868105 0.29375361] [0.08666713 0.43133631]] [[0.93674358 0.50061816] [0.58272102 0.40495383] [0.94631605 0.51712061]]]) 三、numpy.random.randn()返回值具有标准正太分N(0,1)语法：1234numpy.random.randn(d0,d1,...,dn)# randn函数返回一个或一组样本，具有标准正态分布。# dn表格每个维度# 返回值为指定维度的array 例子：1234567891011121314151617181920212223242526np.random.randn() # 当没有参数时，返回单个数据# output0.10125281554614712np.random.randn(2,4)# outputarray([[ 0.88550167 0.94488988 0.68380494 -2.42685228] [-0.12320336 -0.21325373 0.07526319 -0.59034683]])np.random.randn(4,3,2)# outputarray([[[ 1.09663108 -0.58652581] [-0.94125426 0.01259422] [-0.4189963 0.03315681]] [[-0.38388522 -0.46672398] [ 0.35335792 -0.92017071] [-0.26904716 2.28057168]] [[-0.37893325 -0.64560053] [ 0.93308775 0.68629363] [ 2.6449362 1.40476443]] [[ 1.90709637 0.18952578] [-0.09167653 1.95480404] [-1.55128951 -0.07542736]]]) 四、numpy.random.randint()4.1、 numpy.random.randint()语法：12345numpy.random.randint(low, high=None, size=None, dtype=’l’) 返回随机整数，范围区间为[low,high），包含low，不包含high 参数：low为最小值，high为最大值，size为数组维度大小，dtype为数据类型，默认的数据类型是np.int high没有填写时，默认生成随机数的范围是[0，low) 例子：123456789101112np.random.randint(1,size=5) # 返回[0,1)之间的整数，所以只有0# outputarray([0, 0, 0, 0, 0])np.random.randint(1,5) # 返回1个[1,5)时间的随机整数# output3np.random.randint(-5,5,size=(2,2))# outputarray([[-4 -5] [-2 -2]]) 4.2、 numpy.random.random_integers语法：1234567numpy.random.random_integers(low, high=None, size=None) 返回随机整数，范围区间为[low,high]，包含low和high 参数：low为最小值，high为最大值，size为数组维度大小 high没有填写时，默认生成随机数的范围是[1，low]该函数在最新的numpy版本中已被替代，建议使用randint函数 五、生成[0,1)之间的浮点数语法：1234numpy.random.random_sample(size=None)numpy.random.random(size=None)numpy.random.ranf(size=None)numpy.random.sample(size=None) 例子：12345678910111213141516171819202122print('-----------random_sample--------------')print(np.random.random_sample(size=(2,2)))print('-----------random--------------')print(np.random.random(size=(2,2)))print('-----------ranf--------------')print(np.random.ranf(size=(2,2)))print('-----------sample--------------')print(np.random.sample(size=(2,2)))# output-----------random_sample--------------[[0.73845538 0.57461322] [0.50090024 0.12809794]]-----------random--------------[[0.65293998 0.15751317] [0.0745293 0.03874293]]-----------ranf--------------[[0.75927288 0.79548232] [0.47618339 0.04334719]]-----------sample--------------[[0.81010338 0.59291716] [0.58648785 0.1767814 ]] 六、 numpy.random.choice()语法：12345numpy.random.choice(a, size=None, replace=True, p=None) 从给定的一维数组中生成随机数 参数： a为一维数组类似数据或整数；size为数组维度；p为数组中的数据出现的概率 a为整数时，对应的一维数组为np.arange(a) 例子：123456789101112131415161718192021np.random.choice(5,3)# outputarray([4 4 3])np.random.choice(5, 3, replace=False)# 当replace为False时，生成的随机数不能有重复的数值# outputarray([4 2 0])np.random.choice(5,size=(3,2))# output[[0 0] [1 3] [0 4]]demo_list = ['lenovo', 'sansumg','moto','xiaomi', 'iphone']np.random.choice(demo_list,size=(3,3))# outputarray([['sansumg' 'iphone' 'moto'] ['moto' 'iphone' 'iphone'] ['xiaomi' 'iphone' 'xiaomi']]) 参数p的长度与参数a的长度需要一致； 参数p为概率，p里的数据之和应为1 123456demo_list = ['lenovo', 'sansumg','moto','xiaomi', 'iphone']np.random.choice(demo_list,size=(3,3), p=[0.1,0.6,0.1,0.1,0.1])# outputarray([['moto' 'sansumg' 'xiaomi'] ['lenovo' 'moto' 'sansumg'] ['sansumg' 'sansumg' 'sansumg']]) 七、numpy.random.seed()语法： np.random.seed()的作用：使得随机数据可预测。 当我们设置相同的seed，每次生成的随机数相同。如果不设置seed，则每次会生成不同的随机数 1234567891011121314np.random.seed(0)np.random.rand(5)# outputarray([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])np.random.seed(1676)np.random.rand(5)# outputarray([0.39983389, 0.29426895, 0.89541728, 0.71807369, 0.3531823 ])np.random.seed(1676)np.random.rand(5)# outputarray([0.39983389, 0.29426895, 0.89541728, 0.71807369, 0.3531823 ]) 八、结束语嗯？～～","tags":[{"name":"Numpy Random","slug":"Numpy-Random","permalink":"http://heany.github.io/tags/Numpy-Random/"}]},{"title":"Nginx+PHP+MySQL搭建过程","date":"2018-05-15T08:43:10.000Z","path":"2018/05/15/Nginx-PHP-MySQL搭建过程/","text":"一、 前言&emsp;最近要开始干活了～老板要求边做边写开发文档，啊啊啊啊，就把这一系列的文档发在博客里面吧，需要的时候再来拿，还能凑几篇博客。这是第一篇，关于Nginx+php+MySQL环境搭建的，很简单网上很多～～～ 二、 本机环境 OS: Ubuntu 17.10.1_x64 Nginx_version: nginx/1.12.1 PHP_version: PHP-7.1.17 MySQL_version: mysql5.7.22 三、 安装步骤3.1、 安装Nginx Web Server直接使用apt package management来完成安装，命令如下 12345678sudo apt-get updatesudo apt-get upgradesudo apt-get install nginx# 等一会儿就安装好了，下面是一些重要的文件目录nginx的配置文件目录： /etc/nginx;Nginx的缓存目录： /var/cache/nginx;Nginx的日志目录： /var/log/nginx; 启动nginx服务 sudo systemctl start nginx test #打开网页，输入http://localhost可以看到welcome to nginx字样 3.2、 安装MySQL管理网站数据同样是使用ubuntu中的apt包管理器来安装，命令如下：12sudo apt-get install mysql-server# 会弹框提醒输入root密码，直接输入然后按`Enter`，结束。 3.3、 安装PHP服务同样是使用ubuntu中的apt包管理器来安装，命令如下：12345# 顺便把php-mysql插件装上sudo apt-get install php-fpm php-mysql# 一些重要的文件目录# php配置文件路径： /etc/php/7.1/fpm/php.ini 然后，修改php的配置文件：12345678# 切换到php配置目录cd /etc/php/7.1/fpm# 修改配置文件sudo vim php.ini++++找到这行，取消注释，将1改成0cgi.fix_pathinfo=0++++ 最后，重启php服务1sudo systemctl restart php-7.1-fpm 3.4、 配置Nginx去使用PHP处理器12345678910111213141516171819202122232425262728# 切换到nginx网站的配置目录cd /etc/nginx/sites-availablesudo vim default# 做如下修改：++++server &#123; listen 80 default_server; listen [::]:80 default_server; root /var/www/html; index index.php index.html index.htm index.nginx-debian.html; server_name server_domain_or_IP; location / &#123; try_files $uri $uri/ =404; &#125; location ~ \\.php$ &#123; include snippets/fastcgi-php.conf; fastcgi_pass unix:/run/php/php7.1-fpm.sock; &#125; location ~ /\\.ht &#123; deny all; &#125;&#125;++++ 这里有个小坑，了解一下：由于安装的php版本是php7.1，安装nginx默认配置文件里面使用的是/run/php/php7.0-fpm.sock,你在/run/php中找不到php7.0的sock文件。所以，这里我们要把它修改为我们安装的php相应版本的sock文件。即/run/php/php7.1-fpm.sock; 然后，测试配置文件是否生效。 sudo nginx -t #如果没报错，说明配置成功 最后，重新加载nginx服务器 sudo systemctl reload nginx 3.5、 测试nginx和PHP配置是否生效在网站的根目录/var/www/html下新建一个info.php文件，写入一下内容：1234// info.php&lt;?phpphpinfo();?&gt; 然后，打开浏览器，输入localhost/info.php，会看到输出了之前安装的php的版本信息和其他模块的详细信息。这 就 表 明 Nginx 和PHP都安装和配置～～成功了！！！ 四、 结束语路很长～这才刚刚开始啊～～～～ 参考自博客","tags":[{"name":"Nginx-php-MySQL","slug":"Nginx-php-MySQL","permalink":"http://heany.github.io/tags/Nginx-php-MySQL/"}]},{"title":"TF-IDF理解","date":"2018-05-14T07:38:47.000Z","path":"2018/05/14/TF-IDF理解/","text":"一、 前言&emsp;最近在看关于特征抽取的论文，希望能从中找到一些方法来做数据抄袭检测，看到了一篇关于中文文本复制检测的文章，里面用到了TF-IDF算法，查找了些资料，把它整理了一下～～ 二、 TF-IDF的理解&emsp;TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与资讯探勘的常用加权技术, TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TFIDF实际上是：TF * IDF，TF词频(Term Frequency)，IDF反文档频率(Inverse Document Frequency)。TF表示词条在文档d中出现的频率。IDF的主要思想是：如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。如果某一类文档C中包含词条t的文档数为m，而其它类包含t的文档总数为k，显然所有包含t的文档数n=m + k，当m大的时候，n也大，按照IDF公式得到的IDF的值会小，就说明该词条t类别区分能力不强。但是实际上，如果一个词条在一个类的文档中频繁出现，则说明该词条能够很好代表这个类的文本的特征，这样的词条应该给它们赋予较高的权重，并选来作为该类文本的特征词以区别与其它类文档。这就是IDF的不足之处。 TF公式： 以上式子中$n_i,j$是该词在文件$d_j$中出现的次数，而分母则是在文件$d_j$中所有字词的出现次数之和。 IDF公式： 三、 简单案例理解假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。一个计算文件频率 (DF) 的方法是测定有多少份文件出现过“母牛”一词，然后除以文件集里包含的文件总数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是 lg(10,000,000 / 1,000)=4。最后的TF-IDF的分数为0.03 * 4=0.12。 四、 结束语markdown写公式没有加载出来，不知道咋回事～可能模板的锅。 LaTex写公式真方便啊～～～","tags":[{"name":"TF-IDF-Algorithm","slug":"TF-IDF-Algorithm","permalink":"http://heany.github.io/tags/TF-IDF-Algorithm/"}]},{"title":"删除Github.com上repository中的某个文件夹","date":"2018-04-18T11:28:46.000Z","path":"2018/04/18/删除Github-com上repository中的某个文件夹/","text":"一、 前言&emsp;最近上传项目的时候，粗心大意～忘记将一些需要忽略的文件夹加进.gitignore文件中！！！导致把项目中所有文件都上传到github.com上去了，其中有一些文件我并不想上传上去。所以呢？我现在想要把一个文件夹（例如bigData）在github删除，但是又不想把本地的bigData文件夹删掉～但是！github上面只能删除文件（如下图所示），并不支持删除文件夹。 二、采用git指令来进行删除操作代码如下所示：（以删除bigData文件夹为例）1234567# 首先切换到本地工作目录，也就是与github上对应的那个目录。cd dir/# 然后执行下面的git命令git rm -r --cached bigData # --cached不会把本地的bigData删除git commit -m \"delete bigData dir\"git push -u origin master","tags":[{"name":"git","slug":"git","permalink":"http://heany.github.io/tags/git/"}]},{"title":"Docker技术学习","date":"2018-04-16T07:20:03.000Z","path":"2018/04/16/Docker技术学习/","text":"一、 前言我们都知道软件开发最大的麻烦事之一，就是环境配置。用户计算机的环境都不相同，你怎么知道自家的软件，能在那些机器跑起来？ 用户必须保证两件事：操作系统的设置，各种库和组件的安装。只有它们都正确，软件才能运行。举例来说，安装一个 Python 应用，计算机必须有 Python 引擎，还必须有各种依赖，可能还要配置环境变量。 如果某些老旧的模块与当前环境不兼容，那就麻烦了。开发者常常会说：”它在我的机器可以跑了”（It works on my machine），言下之意就是，其他机器很可能跑不了。 环境配置如此麻烦，换一台机器，就要重来一次，旷日费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。 虚拟机（virtual machine）就是带环境安装的一种解决方案。它可以在一种操作系统里面运行另一种操作系统，比如在 Windows 系统里面运行 Linux 系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。虽然用户可以通过虚拟机还原软件的原始环境，但是虚拟机却有很多缺点，资源占用较多，拖慢计算机的运行速度，冗余步骤多;启动很慢等。结构对比如下图所示： 由于虚拟机存在这些缺点，Linux 发展出了另一种虚拟化技术：Linux 容器（Linux Containers，缩写为 LXC）。 Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离。或者说，在正常进程的外面套了一个保护层。对于容器里面的进程来说，它接触到的各种资源都是虚拟的，从而实现与底层系统的隔离。由于容器是进程级别的，相比虚拟机有很多优势。例如：启动很快，资源占用少;体积很小。容器就像是轻量级的虚拟机，能够提供虚拟化的环境，但是成本开销比虚拟机要小得多。 二、 Docker是什么？Docker属于Linux容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。 Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。 总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 三、 基本概念Docker包括三个基本概念： 镜像（Image） 容器（Container） 仓库（Repository） 理解了这三个概念，也就理解了Docker的整个生命周期。 3.1、 Docker镜像（Image）我们都知道,操作系统分为内核和用户空间。对于Linux而言,内核启动后,会挂载root文件系统为其提供用户空间支持。而 Docker 镜像(Image),就相当于是一个文件系统。比如官方镜像ubuntu:16.04就包含了完整的一套Ubuntu 16.04最小系统的root文件系统。 Docker 镜像是一个特殊的文件系统,除了提供容器运行时所需的程序、库、资源、配置等文件外,还包含了一些为运行时准备的一些配置参数(如匿名卷、环境变量、用户等)。镜像不包含任何动态数据,其内容在构建之后也不会被改变。 分层存储因为镜像包含操作系统完整的root文件系统,其体积往往是庞大的,因此在Docker设计时,就充分利用Union FS的技术,将其设计为分层存储的架构。所以严格来说,镜像并非是像一个ISO那样的打包文件,镜像只是一个虚拟的概念,其实际体现并非由一个文件组成,而是由一组文件系统组成,或者说,由多层文件系统联合组成。 镜像构建时,会一层层构建,前一层是后一层的基础。每一层构建完就不会再发生改变,后一层上的任何改变只发生在自己这一层。比如,删除前一层文件的操作,实际不是真的删除前一层的文件,而是仅在当前层标记为该文件已删除。在最终容器运行的时候,虽然不会看到这个文件,但是实际上该文件会一直跟随镜像。因此,在构建镜像的时候,需要额外小心,每一层尽量只包含该层需要添加的东西,任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层,然后进一步添加新的层,以定制自己所需的内容,构建新的镜像。 3.2、 Docker容器镜像(Image)和容器(Container)的关系,就像是面向对象程序设计中的类和实例一样,镜像是静态的定义,容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 容器的实质是进程,但与直接在宿主执行的进程不同,容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的root文件系统、自己的网络配置、自己的进程空间,甚至自己的用户ID空间。容器内的进程是运行在一个隔离的环境里,使用起来,就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性,很多人初学Docker时常常会混淆容器和虚拟机。 前面讲过镜像使用的是分层存储,容器也是如此。每一个容器运行时,是以镜像为基础层,在其上创建一个当前容器的存储层,我们可以称这个为容器运行时读写而准备的存储层为容器存储层。 容器存储层的生存周期和容器一样,容器消亡时,容器存储层也随之消亡。因此,任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求,容器不应该向其存储层内写入任何数据,容器存储层要保持无状态化。所有的文件写入操作,都应该使用数据卷(Volume)、或者绑定宿主目录,在这些位置的读写会跳过容器存储层,直接对宿主(或网络存储)发生读写,其性能和稳定性更高。 数据卷的生存周期独立于容器,容器消亡,数据卷不会消亡。因此,使用数据卷后,容器删除或者重新运行之后,数据却不会丢失。 3.3、 Docker仓库（Registry）镜像构建完成后,可以很容易的在当前宿主机上运行,但是,如果需要在其它服务器上使用这个镜像,我们就需要一个集中的存储、分发镜像的服务,Docker Registry 就是这样的服务。 一个Docker Registry中可以包含多个仓库(Repository);每个仓库可以包含多个标签(Tag);每个标签对应一个镜像。 通常,一个仓库会包含同一个软件不同版本的镜像,而标签就常用于对应该软件的各个版本。我们可以通过&lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签,将以latest作为默认标签。 以Ubuntu镜像 为例,ubuntu是仓库的名字,其内包含有不同的版本标签,如,14.04,16.04。我们可以通过ubuntu:14.04,或者ubuntu:16.04来具体指定所需哪个版本的镜像。如果忽略了标签,比如ubuntu,那将视为ubuntu:latest。 仓库名经常以两段式路径形式出现,比如jwilder/nginx-proxy,前者往往意味着Docker Registry多用户环境下的用户名,后者则往往是对应的软件名。但这并非绝对,取决于所使用的具体Docker Registry的软件或服务。 3.3.1、 Docker Registry 公开服务Docker Registry公开服务是开放给用户使用、允许用户管理镜像的Registry服务。一般这类公开服务允许用户免费上传、下载公开的镜像,并可能提供收费服务供用户管理私有镜像。 最常使用的 Registry公开服务是官方的Docker Hub,这也是默认的Registry,并拥有大量的高质量的官方镜像。除此以外,还有CoreOS的Quay.io,CoreOS相关的镜像存储在这里;Google的Google Container Registry,Kubernetes的镜像使用的就是这个服务。 由于某些原因,在国内访问这些服务可能会比较慢。国内的一些云服务商提供了针对Docker Hub的镜像服务(Registry Mirror),这些镜像服务被称为加速器。常见的有阿里云加速器、DaoCloud加速器 等。使用加速器会直接从国内的地址下载Docker Hub的镜像,比直接从Docker Hub下载速度会提高很多。 国内也有一些云服务商提供类似于Docker Hub的公开服务。比如时速云镜像仓库、网易云镜像服务、DaoCloud像市场、阿里云镜像库 等。 3.3.2、 私有 Docker Registry除了使用公开服务外,用户还可以在本地搭建私有Docker Registry。Docker官方提供了Docker Registry镜像,可以直接使用做为私有Registry服务。 开源的Docker Registry镜像只提供了Docker Registry API的服务端实现,足以支持docker命令,不影响使用。但不包含图形界面,以及镜像维护、用户管理、访问控制等高级功能。在官方的商业化版本Docker Trusted Registry中,提供了这些高级功能。 除了官方的Docker Registry外,还有第三方软件实现了Docker Registry API,甚至提供了用户界面以及一些高级功能。比如,VMWare Harbor和Sonatype Nexus。 四、 Docker的安装Docker是一个开源的商业产品。Docker CE（社区版）安装参考官方文档。 Ubuntu 使用了国内源12345678910111213141516171819202122232425sudo apt-get updatesudo apt-get install \\apt-transport-https \\ca-certificates \\curl \\software-properties-commoncurl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add sudo add-apt-repository \\\"deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \\$(lsb_release -cs) \\stable\"sudo apt-get updatesudo apt-get install docker-ce# 启动dockersudo systemctl enable dockersudo systemctl start docker# 建立docker用户组sudo groupadd dockersudo usermod -aG docker $USER 安装完成后，需配置国内镜像加速,参考这篇文章： 在/etc/docker/daemon.json中写入如下内容（如果文件不存在请新建该文件） { &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ] } 之后重新启动docker服务： sudo systemctl daemon-reload sudo systemctl restart docker 检查加速器是否生效： sudo docker info # 看到如下内容说明配置成功 Registry Mirrors: https://registry.docker-cn.com/ 测试Docker是否安装正确123456789101112131415docker run hello-world# outputUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-world9bb5a5d4561a: Pull complete Digest: sha256:f5233545e43561214ca4891fd1157e1c3c563316ed8e237750d59bde73361e77Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. ........... 五、 Image文件Docker把应用程序及其依赖，打包在 image 文件里面。只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 image 是二进制文件。实际开发中，一个image文件往往通过继承另一个image文件，加上一些个性化设置而生成。举例来说，你可以在 Ubuntu 的 image 基础上，往里面加入 Apache 服务器，形成你的image。12345# 列出本机所有的image文件docker image ls# 删除image文件docker image rm [imageName] image 文件是通用的，一台机器的image文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的image文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。 为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库Docker Hub 是最重要、最常用的 image 仓库。此外，出售自己制作的 image 文件也是可以的。 将image文件从仓库抓取到本地。 docker image pull library/hello-world # docker image pull 是抓取image的命令。library/hello-world是image文件在仓库里面的位置，其中library是image文件所在的组，hello-world是image的名字。 由于Docker官方提供的image文件，都放在library组里面，所以它是默认组，可以省略，因此，上面的命令可以写成下面这样： docker image pull hello-world 抓取成功后，就可以在本机看到这个image文件了 docker image ls 运行image文件命令如下： docker container run hello-world 或者 docker run hello-world docker container run或者docker run命令会从image文件，生成一个正在运行的容器实例。如果本地没有找到image文件，docker就会从仓库自动抓取image文件，因此，前面的docker image pull命令不是必需的步骤。 注意:有些容器不会自动终止，必须使用docker container kill命令手动终止。docker container kill [containID] 六、 容器文件 image文件生成的容器实例，本身也是一个文件，称为容器文件。也就是说，一旦容器生成，就会同时存在两个文件：image文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。 # 列出本机正在运行的容器 docker container ls # # 列出本机所有容器，包括终止运行的容器 docker container ls -all # # 运行终止的容器文件，依然会占据硬盘空间，可以使用以下命令删除。 docker container rm [containerID] 学会使用image文件以后，接下来的问题就是，如何可以生成image文件？如果你要推广自己的软件，势必要自己制作image 文件。这就需要用到Dockerfile文件。它是一个文本文件，用来配置image。Docker根据该文件生成二进制的 image 文件。下一章将通过一个实例来介绍如何编写Dockfile文件。 七、 实例：制作自己的Docker容器下面我以koa-demos项目为例，介绍怎么写Dockerfile文件，实现让用户在Docker容器里面运行Koa框架。作为准备工作，请先下载源码，或者采用下面方式下载。 git clone https://github.com/ruanyf/koa-demos.git cd koa-demos 7.1、 编写Dockerfile文件首先，在项目的根目录下，新建一个文本文件.dockerignore,写入下面的内容， 123.gitnode_modulesnpm-debug.log 上面的代码表示，这三个路径要排除，不要打包进入image文件，如果没有要排除的路径，可以不写。 然后，在项目的根目录下，新建一个文本文件Dockerfile。写入下面的内容。12345FROM node:8.4COPY . /appWORKDIR /appRUN npm install --registry=https://registry.npm.taobao.orgEXPOSE 3000 上面五行代码，含义如下。 FROM node:8.4：该image文件继承官方的node image，冒号表示标签，这里标签是8.4，即8.4版本的 node。 COPY . /app：将当前目录下的所有文件（除了.dockerignore排除的路径），都拷贝进入 image 文件的/app目录。 WORKDIR /app：指定接下来的工作路径为/app。 RUN npm install：在/app目录下，运行npm install命令安装依赖。注意，安装后所有的依赖，都将打包进入 image 文件。 EXPOSE 3000：将容器 3000 端口暴露出来， 允许外部连接这个端口。 7.2、 创建image文件有了Dockerfile文件以后，就可以使用docker image build命令创建image文件了。 docker image build -t koa-demo . # 或者 docker image build -t koa-demo:0.0.1 . 在上面的代码中，-t参数用来指定image文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是latet。最后的那个点.表示dockfile文件所在的路径，上例是当前路径，所以是一个点。如果运行成功就可以看到新生成的image文件koa-demo了。 # 查看image文件 docker image ls 7.3、 生成容器docker container run命令会从image文件生成容器。123docker container run -p 8000:3000 -it koa-demo /bin/bash# 或者docker container run -p 8000:3000 -it kao-demo:0.0.1 /bin/bash 上面命令的参数含义如下： -p参数：容器的3000端口映射到本机的8000端口 -it参数：容器的 Shell 映射到当前的 Shell，然后你在本机窗口输入的命令，就会传入容器。 koa-demo:0.0.1：image 文件的名字（如果有标签，还需要提供标签，默认是 latest 标签）。 /bin/bash：容器启动以后，内部第一个执行的命令。这里是启动 Bash，保证用户可以使用 Shell。 如果一切正常，运行上面的命令后，就会返回一个命令行提示符。 root@805724fbb49b:/app# 这就表示你已经在容器里面了，返回的提示符就是容器内部的Shell提示符。执行下面的命令。 root@805724fbb49b:/app# node demos/01.js 这时，Koa 框架已经运行起来了。打开本机的浏览器，访问http://127.0.0.1:8000，网页显示”Not Found”，这是因为这个 demo 没有写路由。 这个例子中，Node 进程运行在 Docker 容器的虚拟环境里面，进程接触到的文件系统和网络接口都是虚拟的，与本机的文件系统和网络接口是隔离的，因此需要定义容器与物理机的端口映射（map）。 现在，在容器的命令行，按下 Ctrl + c 停止 Node 进程，然后按下 Ctrl + d （或者输入 exit）退出容器。此外，也可以用docker container kill终止容器运行。 #在本机的另一个终端窗口，查处容器的ID docker container ls # # 停止指定的容器运行 docker container kill [containerID] 容器停止运行后不会消失，用下面的命令删除容器文件。12345# 查处容器的IDdocker container ls -all# 删除指定容器文件docker container rm [containerID] 也可以使用docker container run命令的--rm参数，在容器终止运行后自动删除容器文件。 docker container run --rm -p 8000:3000 -it koa-demo /bin/bash 7.4、 CMD命令上一节的例子里面，容器启动以后，需要手动输入命令node demos/01.js。我们可以把这个命令写在Dockerfile里面，这样容器启动以后，这个命令就已经执行了，不用再手动输入了。123456FROM node:8.4COPY . /appWORKDIR /appRUN npm install --registry=https://registry.npm.taobao.orgEXPOSE 3000CMD node demos/01.js 上面的 Dockerfile 里面，多了最后一行CMD node demos/01.js，它表示容器启动后自动执行node demos/01.js。 你可能会问，RUN命令与CMD命令的区别在哪里？简单说，RUN命令在image文件的构建阶段执行，执行结果都会打包进入image文件；CMD命令则是在容器启动后执行。另外，一个Dockerfile可以包含多个RUN命令，但是只能有一个CMD命令。 注意，指定了CMD命令以后，docker container run命令就不能附加命令了（比如前面的/bin/bash），否则它会覆盖CMD命令。现在，启动容器可以使用下面的命令。 docker container run --rm -p 8000:3000 -it koa-demo 7.5、 发布image文件容器运行成功后，就确认了 image 文件的有效性。这时，我们就可以考虑把 image 文件分享到网上，让其他人使用。 首先，去 hub.docker.com 或 cloud.docker.com 注册一个账户。然后，用下面的命令登录。 docker login 然后，为本地的image标注用户名和版本。123docker image tag [imageName] [username]/[repository]:[tag]# 实例docker image tag koa-demos:0.0.1 heany/koa-demos:0.0.1 也可以不用标注用户名，重新构建一下image文件。 docker image build -t [username]/[repository]:[tag] 最后，发布image文件。 docker image push [username]/[repository]:[tag] 发布成功后，登录hub.docker.com，就可以看到已经发布的image文件。目前国内链接hub.docker.com非常慢，push经常超时，可以使用国内的daocloud。 八、 其他有用的命令docker的主要用法就是上面这些此外还有几个命令，也非常有用。 8.1、 docker container start前面的docker container run命令是新建容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用docker container start命令，它用来启动已经生成、已经停止运行的容器文件。 docker container start [containerID] 8.2、 docker container stop前面的docker container kill命令终止容器运行，相当于向容器里面的主进程发出SIGKILL 信号。而docker container stop命令也是用来终止容器运行，相当于向容器里面的主进程发出SIGTERM信号，然后过一段时间再发出 SIGKILL 信号。 bash container stop [containerID] 这两个信号的差别是，应用程序收到SIGTERM信号以后，可以自行进行收尾清理工作，但也可以不理会这个信号。如果收到SIGKILL信号，就会强行立即终止，那些正在进行中的操作会全部丢失。 8.3、 docker container logsdocker container logs命令用来查看docker容器的输出，即容器里面Shell的标准输出。如果docker run命令运行容器的时候，没有使用-it参数，就要用这个命令查看输出。 docker container logs [containerID] 8.4、 docker container execdocker container exec命令用于进入一个正在运行的docker容器。如果docker run命令运行容器的时候，没有使用-it参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的Shell执行命令了。 docker container exec -it [containerID] /bin/bash 8.5、 docker container cpdocker container cp命令用于从正在运行的Docker容器里面，将文件拷贝到本机。下面是拷贝到当前目录的写法。 docker container cp [containID]:[/path/to/file] 九、 结束语&emsp;转载于阮一峰的网络日志-Docker入门教程&emsp;参考了Docker技术入门与实战","tags":[{"name":"Docker","slug":"Docker","permalink":"http://heany.github.io/tags/Docker/"}]},{"title":"sublime text 3 最新的注册码","date":"2018-04-09T12:46:12.000Z","path":"2018/04/09/sublime-text-3-最新的注册码/","text":"最新可用的sublime Text 3注册码12345678910111213—– BEGIN LICENSE —– TwitterInc 200 User License EA7E-890007 1D77F72E 390CDD93 4DCBA022 FAF60790 61AA12C0 A37081C5 D0316412 4584D136 94D7F7D4 95BC8C1C 527DA828 560BB037 D1EDDD8C AE7B379F 50C9D69D B35179EF 2FE898C4 8E4277A8 555CE714 E1FB0E43 D5D52613 C3D12E98 BC49967F 7652EED2 9D2D2E61 67610860 6D338B72 5CF95C69 E36B85CC 84991F19 7575D828 470A92AB —— END LICENSE ——","tags":[{"name":"sublime-text-3 注册码","slug":"sublime-text-3-注册码","permalink":"http://heany.github.io/tags/sublime-text-3-注册码/"}]},{"title":"Ubuntu 17.10配置Hadoop+Spark环境","date":"2018-04-08T06:39:34.000Z","path":"2018/04/08/Ubuntu-17-10配置Hadoop-Spark环境/","text":"一、前言&nbsp;&nbsp;最近导师带的项目是与大数据相关，感觉这几年大数据技术还挺火的，就想着也去学一下，丰富自己的技能栈。本文主要讲的是hadoop+spark的环境搭建,然后使用自带的examples测试环境，这里不涉及原理介绍。 二、Hadoop的三种运行模式介绍2.1、 单机模式也叫独立模式（Local或Standalone Mode） 默认情况下，Hadoop即处于该模式，用于开发和调式。 不对配置文件进行修改。 使用本地文件系统，而不是分布式文件系统。 Hadoop不会启动NameNode、DataNode、JobTracker、TaskTracker等守护进程，Map()和Reduce()任务作为同一个进程的不同部分来执行的。 用于对MapReduce程序的逻辑进行调试，确保程序的正确。 2.2、 伪分布式模式（Pseudo-Distrubuted Mode） Hadoop的守护进程运行在本机机器上，模拟一个小规模的集群 在一台主机上模拟多主机。 Hadoop启动NameNode、DataNode、JobTracker、TaskTracker这些守护进程都在同一台机器上运行，是相互独立的Java进程。 在这种模式下，Hadoop使用的是分布式文件系统，各个作业也是由JobTraker服务，来管理的独立进程。在单机模式之上增加了代码调试功能，允许检查内存使用情况，HDFS输入输出，以及其他的守护进程交互。类似于完全分布式模式，因此，这种模式常用来开发测试Hadoop程序的执行是否正确。 修改3个配置文件：core-site.xml（Hadoop集群的特性，作用于全部进程及客户端）、hdfs-site.xml（配置HDFS集群的工作属性）、mapred-site.xml（配置MapReduce集群的属性） 格式化文件系统 2.3、 全分布式集群模式（Full-Distributed Mode） Hadoop的守护进程运行在一个集群上 Hadoop的守护进程运行在由多台主机搭建的集群上，是真正的生产环境。 在所有的主机上安装JDK和Hadoop，组成相互连通的网络。 在主机间设置SSH免密码登录，把各从节点生成的公钥添加到主节点的信任列表。 修改3个配置文件：core-site.xml、hdfs-site.xml、mapred-site.xml，指定NameNode和JobTraker的位置和端口，设置文件的副本等参数 格式化文件系统。 三、搭建伪分布式集群的前提条件3.1、 实验环境 ubuntu 17.10-x64 jdk_1.8.0_162 Hadoop-3.0.0 Spark-2.3.0 3.2、 安装JDK，并配置环境变量&nbsp;&nbsp;首先去官网下载对应系统版本的jdk,然后解压到opt目录下，命令如下： sudo tar -zxvf jdk-8u162-linux-x64.tar.gz -C /opt 然后切换到/opt目录下，修改jdk的文件夹命名 mv jdk-8u162-linux-x64 ./jdk 最后，配置环境变量， vim /etc/profile 在配置文件中加入： 1234export JAVA_HOME=/opt/jdkexport JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/libexport PATH=$PATH:$JAVA_HOME/bin 使配置文件生效，执行下面的命令： source /etc/profile 最后查看是否安装成功 java -version 3.3、 安装Scala 官网下载scala(scala-2.12.4.tgz) 解压下载scala包 123sudo tar -xzvf scala-2.12.4.tgz -C /opt/# 修改文件名：sudo mv scala-2.12.4 ./scala 添加环境变量 1234sudo vim /etc/profile# 在后面添加下面内容export SCALA_HOME=/opt/scalaexport PATH=$SCALA_HOME/bin:$PATH 使配置生效并测试 123source /etc/profile# 测试是否安装成功scala -version # 输出scala版本号 3.4、 安装ssh并设置免密登录 安装ssh。如果已安装则跳过这一步。 1sudo apt-get install openssh-server 配置ssh无密登录 12ssh-keygen -t rsa # 然后一直回车cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 测试ssh无密登录 12ssh localhost# 如果不提示输入密码则配置成功 [TOC] 四、搭建伪分布式集群4.1、 hadoop下载安装 下载Hadoop(笔者下载的是hadoop-3.0.0.tar.gz) 解压并重命名 12345# 解压sudo tar -xzvf hadoop-3.0.0.tar.gz -C /opt/# 重命名cd /optsudo hadoop-3.0.0 hadoop 修改文件权限 123cd /optsudo chown -R yourname:yourname hadoop # yourname替换成你的用户名 -R表示逐级往下授权 配置环境变量 123456sudo vim /etc/profile# 在最后添加下面代码export HADOOP_HOME=/opt/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR\" 使配置生效 1source /etc/profile 测试 1hadoop version # output the information of the version of the hadoop 4.2、 Hadoop伪分布式配置 修改配置文件hadoop-env.sh 123456# 切换到工作目录cd /opt/hadoop/etc/hadoop# 打开配置文件vim hadoop-env.sh# 直接加上下面代码export JAVA_HOME=/opt/jdk 修改配置文件core-site.xml 1234&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://219.223.243.131:9000&lt;/value&gt;&lt;/property&gt; 219.223.243.131是我的节点所在主机的ip,9000为默认的端口，不用更改 修改配置文件hdfs-site.xml 1234567891011121314151617181920&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;hadoop-cluster&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///data/hadoop/hdfs/nn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;file:///data/hadoop/hdfs/snn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt; &lt;value&gt;file:///data/hadoop/hdfs/snn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///data/hadoop/hdfs/dn&lt;/value&gt;&lt;/property&gt; 修改配置文件mapred-site.xml 12345678910111213141516171819202122232425262728293031323334&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.admin.user.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; /opt/hadoop/etc/hadoop, /opt/hadoop/share/hadoop/common/*, /opt/hadoop/share/hadoop/common/lib/*, /opt/hadoop/share/hadoop/hdfs/*, /opt/hadoop/share/hadoop/hdfs/lib/*, /opt/hadoop/share/hadoop/mapreduce/*, /opt/hadoop/share/hadoop/mapreduce/lib/*, /opt/hadoop/share/hadoop/yarn/*, /opt/hadoop/share/hadoop/yarn/lib/* &lt;/value&gt;&lt;/property&gt; 修改配置文件yarn-site.xml 1234567891011121314151617181920212223&lt;!-- 指定ResourceManager的地址--&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;219.223.243.131&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定reducer获取数据的方式--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;file:///data/hadoop/yarn/nm&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;4&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 创建相关目录 12345678sudo mkdir -p /data/hadoop/hdfs/nnsudo mkdir -p /data/hadoop/hdfs/dnsudo mkdir -p /data/hadoop/hdfs/snnsudo mkdir -p /data/hadoop/yarn/nm# 然后给这些目录设置读写权限，# 如果使用当前的用户（非root用户）启动相关进程，`/data`必须具有相应的读写权限# 给`/data`目录及其子目录设置读写权限。 -R 递归设置权限sudo chmod -R 777 /data 对HDFS集群进行格式化，HDFS集群是用来存储数据的 1hdfs namenode -format 4.3、 启动集群 启动HDFS集群 1234567# 启动主节点hdfs --daemon start namenode # 启动从节点hdfs --daemon start datanode # 验证节点是否启动，输入一下命令jps # 看是否出现namenode和datanode 启动YARN集群 123456# 启动资源管理器yarn --daemon start resourcemanager# 启动节点管理器yarn --daemon start nodemanager# 验证是否启动，同样是采用`jps`命令，看是否出想相关进程 启动作业历史服务器 12mapred --daemon start historyserver# 同样采用`jps`命令查看是否启动成功 HDFS和YARN集群都有对应的WEB监控页面。 HDFS: http://ip:9870 或者 localhost:9870 YARN: http://ip:8088 HDFS集群的简单操作命令 123hdfs dfs -ls / # 相当于shell中的 llhdfs dfsadmin -safemode leave # 关闭安全模式hdfs dfs -mkdir -p /user/input # 在hdfs文件系统上级联创建文件/user/input， YARN集群examples测试 1234567891011# 计算PI值的作业yarn jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi 4 100# wordcount例子# 首先创建一个输入文件，并上传到hdfs文件系统上hdfs dfs -put input /user/heany# 然后执行yarn jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount input output# 用命令行查看结果hdfs dfs -cat /user/output/part-r-00000# 或者在网页上查看 五、 安装Spark5.1、 下载安装Spark 下载Spark(笔者下载的是spark-2.3.0-bin-hadoop2.7.tgz) 解压下载的Spark包 1sudo tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C /opt 重命名 123# 切换到工作目录cd /optsudo mv spark-2.3.0-bin-hadoop2.7 spark 添加环境变量 1234sudo vim /etc/profile# 在最后添加下面的代码export SPARK_HOME=/opt/sparkexport PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH 修改文件的权限 12cd /optsudo chown -R yourname:yourname ./spark 5.2、 修改配置文件123456789# 拷贝配置文件cd /opt/sparkcp ./conf/spark-env.sh.template ./conf/spark-env.sh# 修改配置文件vim ./conf/spark-env.sh# 在最后添加下面的代码export SPARK_DIST_CLASSPATH=$(/opt/hadoop/bin/hadoop classpath)export JAVA_HOME=/opt/jdk 5.3、运行examples验证安装123/opt/spark/bin/run-example SparkPi 2&gt;&amp;1 | grep \"Pi is roughly\"# output# Pi is roughly 3.143635718178591 5.4、 脚本启动Hadoop和Spark 启动spark 1/opt/spark/sbin/start-all.sh 通过WEB页面查看 浏览器输入地址：http://localhost:8080 编写自动化脚本 12345678910111213141516171819202122232425# 启动Hadoop以及Spark脚本#!/bin/bash# 启动Hadoop以及yarnstart-dfs.shstart-yarn.sh# 启动历史服务器#mr-jobhistory-daemon.sh start historyservermapred --daemon start historyserver# 启动Spark/opt/spark/sbin/start-all.sh# 停止Hadoop以及Spark脚本#!/bin/bash# 停止Hadoop以及yarnstop-dfs.shstop-yarn.sh# 停止历史服务器#mr-jobhistory-daemon.sh stop historyservermapred --daemon stop historyserver# 停止Spark/opt/spark/sbin/stop-all.sh","tags":[{"name":"Ubuntu 17.10 hadoop Spark","slug":"Ubuntu-17-10-hadoop-Spark","permalink":"http://heany.github.io/tags/Ubuntu-17-10-hadoop-Spark/"}]},{"title":"Shell Script学习笔记","date":"2018-03-31T06:09:19.000Z","path":"2018/03/31/Shell-Script学习笔记/","text":"一、 前言最近开始使用Linux系统，经常操作terminal，经常使用Linux下的bash来进行一些重复的操作，因此想学习shell script来简化一些安装软件的步骤，实现自动化部署一些应用，节约时间。本文介绍了一些bash命令的一些技巧，然后详细地讲解了正则表达式与文件格式化处理，并且对于每一个操作都有相关的实例进行练习，加强理解;shell script，同样对于每一个功能都有相应的实例进行练习，可以说～非常适合新手来阅读学习了！ 二、 基础知识&nbsp;&nbsp;什么是 shell script (程序化脚本) 呢？就字面上的意义，我们将他分为两部份。 在『 shell 』部分，我们在bash当中已经提过了，那是一个文字介面底下让我们与系统沟通的一个工具介面。那么『 script 』是啥？ 字面上的意义， script 是『脚本、剧本』的意思。整句话是说， shell script 是针对 shell 所写的『剧本！』&nbsp;&nbsp;什么东西啊？其实， shell script 是利用 shell 的功能所写的一个『程序 (program)』，这个程序是使用纯文字档，将一些 shell 的语法与命令(含外部命令)写在里面， 搭配正规表示法、管线命令与数据流重导向等功能，以达到我们所想要的处理目的。&nbsp;&nbsp;shell script 更提供阵列、回圈、条件与逻辑判断等重要功能，让使用者也可以直接以 shell 来撰写程序，而不必使用类似 C 程序语言等传统程序撰写的语法呢！ 2.1 一些shell命令的基础知识及技巧&nbsp;&nbsp;一些快捷键：^A :鼠标光标移到命令最左端^E :鼠标光标移到命令最右端^k :删除鼠标光标后面的命令^u :删除鼠标光标前面的命令^s :锁定terminal^Q :解锁^y :撤销上一次操作^D :输入结束（EOF），例如邮件结束的时候^Z :暂停目前的命令 cat命令的一些技巧：以下代码将内容写入file12345cat &lt;&lt; EOF &gt; file&gt; hello&gt; world&gt; ......&gt; EOF 管道及tee 管道: 管道的作用是将前一条命令的输出变成管道后命令的输入 ls /bin | wc -l ##统计 ls /bin命令输出的行数 系统中错误的输出是无法通过管道的 用 2&gt;&amp;1 可以把错误的输出编号由2变成1 tee: tee复制输出到指定位置 data | tee file |wc -l ##tee命令复制date命令的输出到file中，并统计输出行数 2.2 脚本中调用其他的解释器(如python)执行的代码&nbsp;&nbsp;shell script中第一行通常需要一行指令指定执行此脚本的解释器,如： #!/bin/bash #!/bin/python #!/bin/perl 以上分别指定了常用的三种脚本的解释器， 多条命令执行 用;号分隔，这种方式各条命令之间没有逻辑性 cd; ls; ll 用逻辑连接符&amp;&amp; ||来连接。 ./confirgure &amp;&amp; make &amp;&amp; make install ##必须前面的命令执行成功，才能执行后面的命令123if con1 com2else com3 上面这段代码相当于 com1 &amp;&amp; com2 &amp;&amp; com3 shell通配符 * :任意个? :1个[]:匹配括号里面的一个 echo输出颜色文本 echo -e &quot;\\e[1;31m This is red text. \\e[0m&quot; printf格式化输出 printf “hello\\n” read命令变量键盘读取使用方法如下： read [-pt] variable paras: -p :后面接提示符 -t :后面接等待的“秒数” example: read atest read -p &quot;please a string:&quot; -t 30 atest declare/typeset命令：声明变量的类型：默认类型为字符串 declare [-aixr] variable paras: -a :将后面的variable的变量定义为数组（array）类型 -i :定义为integer类型 -x :用法与export一样，将后面的变量变成环境变量 -r :将变量设置成为readonly类型，该变量不可被更改，也不能重设 example: sum=100+300+50 echo $sum # sum=100+300+50 declare -i sum=100+300+50 echo $sum #450 array数组变量类型;读取时：${数组} var[index]=content example: var[1]=&quot;Tom&quot; var[2]=&quot;tony&quot; echo &quot;${var[1]},${var[2]}&quot; #Tom,tony alias、unalias命令别名设置：; alias lm=’ls -l | more’ #lm执行的是ls -all more unalias lm #去掉lm命令的别名 history查询历史命令 history [n] history [-c] history [-raw] paras: n :数字，列出最近的n条命令行的意思 -c ：将目前的shell中的所有history内容全部消除 -a : 将新增的history命令新增进去histfiles,或默认写入~/.bash_history -r : 将histfiles的内容读到目前这个shell的history中 -w : 将目前的history记忆内容写进histfiles中。 另： ！number ：执行第几条命令的意思 !command ：由最近的命令向前搜寻命令串开头command的那个命令，并执行 !! : 执行上一个命令 2.3 数据流重定向(redirect) 数据流重定向就是将某个命令执行后应该要出现在屏幕上的数据传输到其他的地方。 标准输入(stdin):代码为0，使用&lt;(覆盖)或&lt;&lt;(追加) 标准输出(stdout)：代码为1，使用&gt;或&gt;&gt; 标准错误输出(stderr):代码为2，使用2&gt;或2&gt;&gt; /dev/null垃圾桶黑洞设备 find /home -name .bashrc 2&gt; /dec/null #将错误的数据丢弃，屏幕上显示正确的数据 ## /home/heany/.bashrc standard input:&lt;与&lt;&lt; cat &gt; catfile &lt; ~/.bashrc #用stdin替代键盘的输入以创建新文件的简单流程 cat &gt; catfile &lt;&lt; &quot;eof&quot; &gt; This is a test. &gt; OK now stop &gt; eof #输入这个关键字，立刻就结束不需要输入[ctrl+D] 2.4 管道命令(pipe) 管道命令使用|这个界定符号，仅能处理经由前面一个命令传来的正确的信息，也就是standard output的信息，对于standard error并没有直接处理的能力。管道后面接的第一个数据必定是”命令“，而且这个命令必须要能够接收standard input的数据才行，这样的命令才可以是“管道命令”，例如less, more,head, tail等。而ls, cp ,mv这些则不是管道命令，因为他们不会接收来自stdin的数据。 2.4.1 选取命令：cut,grep 选取命令就是将一段数据经过分析后，取出我们所想要的，或者是经由分析关键字，得我们所想要的哪一行，通常是针对”行”来分析的。 cut这个命令可以将一段信息的某一段“切出来”，处理信息以“行”为单位。 cut -d &apos;分隔字符&apos; -f fields #用于分隔字符 cut -c 字符范围 #用于排列整齐的信息 paras: -f ：依据-d的分隔字符将一段信息切割成数段，用-f取出第几段的意思。 -c ：以字符的单位取出固定字符区间 examples: echo $PATH | cut -d &apos;:&apos; -f 5 ## 将echo $PATH结果以&quot;:&quot;分隔，，找到第5个 export | cut -c 12- ## 将export输出的信息取得第12字符后的所有字符串 grep命令是分析一行信息，若当中有我们需要的信息，就将该行拿出来。 grep [-acinv] [–color=auto] ‘查找字符串’ filename paras: -a:将binary文件以text文件的形式查找数据 -c:计算找到”查找字符串”的次数 -i:忽略大小写的不同，所以大小写视为相同 -n:顺便输出行号 -v: 反向选择，即显示没有”查找字符串”内容的那一行 –color=auto : 可以将找到的关键字部分加上颜色显示 examples: last | grep ‘root’ #将last中有出现root的那一行就取出来 last | grep -v ‘root’ # 与上面相反，只要没有root就取出 last | grep ‘root’ | cut -d ‘’ -f 1 #在取出root后，利用上一个命令cut的处理，就能取出第一列 2.4.2 排序命令：sort, wc ,uniq sort排序 sort [-fbMnrtuk] [file or stdin] paras: -f : 忽略大小写的差异 -b : 忽略最前面的空格符部分 -M : 以月份的名字来排序 -n : 使用”纯数字“进行排序 -r : 反向排序 -u : 就是uniq，相同的数据中，仅出现一行代表 -t : 分隔符，默认是用[Tab]键来分隔 -k : 以那个区间(field)来进行排序的意思 examples: cat /etc/passwd | sort -t &apos;:&apos; -k 3 #以:来分隔，以第三列来排序 last | cut -d &apos;&apos; -f1 | sort #利用last将输出的数据仅取账号，并加以排序 uniq将重复的数字仅列出一个显示 uniq [-ic] paras: -i : 忽略大小写字符的不同 -c : 进行计数 examples: last | cut -d &apos;&apos; -f1 | sort |uniq # 使用last将账号列出，仅取出账号列，进行排序后取出最后一位 wc计算输出信息的整体数据 wc [-lwm] paras: -l : 仅列出行 -w : 仅列出多少字 -m : 多少字符 examples: cat /etc/man.config | wc #列出config里面到底有多少相关行数、字数、字符数，分别输出三列 2.4.3 双向重定向：tee*tee*会同时将数据流送与文件于屏幕，而输出到屏幕的就是standout,可以让下个命令继续处理。 tee [-a] file paras: -a : 以累加(append)的方式，将数据加入file当中。 examples: last | tee [-a] last.list | cut -d &apos;&apos; -f1 #将last的输出存一份last.list文件中 2.4.4 字符转换命令：tr, col, join, paste, expand主要介绍这些字符转换命令在管道中的使用方法 tr可以用来删除一段信息当中的文字，或者是进行文字信息的替换 tr [-ds] SET1 .... paras: -d : 删除信息当中的SET1这个字符串， -s : 替换掉重复的字符串 examples: last | tr [a-z] [A-Z] #将last输出的信息中所有的小写字符变成大写字符 col col [-xb] paras: -x : 将Tab键转换成对等的空格键 -b : 在文字内有反斜杠\\时，仅保留反斜杠最后接的那个字符 join处理两个文件之间的数据，而且主要是将两个文件当中有相同数据的那一行加在一起。 join [-ti12] file1 file2 paras: -t : join默认以空格符分隔数据，并且对比”第一个字段“的数据，如果两个文件相同，则将两条数据连成一行，且第一个字段放在第一个。 -i : 忽略大小写的差异 -1 : 这个是数字的1,代表第一个文件要用哪个字段来分析的意思 -2 : 代表第二个文件要用哪个字段来分析的意思。 examples: join -t &apos;:&apos; /etc/passwd /etc/shadow #将两个文件第一个字段相同者整合成一行 paste直接将两行贴在一起，且中间一[tab]键隔开。 paste [-d] file1 file2 paras: -d : 后面可以接分隔字符，默认是以[tab]来分隔的 - : 如果file部分写成 - ，表示来自standard input的数据的意思 examples: paste /etc/passwd /etc/shadow # 将两个文件的同一行贴在一起 2.4.5 切割命令：split切割命令将一个大文件依据文件大小或行数来切割成为小文件 split [-bl] file PREFIX paras: -b : 后面可接欲切割成的文件大小，可加单位 -l : 以行数来进行切割 PREFIX : 代表前导符，可作为切割文件的前导文字。 examples: cd /tmp; split -b 300k /etc/termcap termcap # 将/etc/termcap分成300K一个文件 ls -al / | split -l 10 -lsroot #将使用ls-al /输出的信息中，每10行记录成一个文件 三、 正则表达式与文件格式化处理3.1、 正则表达式概念(Regular Expression) 简单的说,正则表达式就是处理字串的方法,他是以行为单位来进行字串的处理行为, 正则表达式通过一些特殊符号的辅助,可以让使用者轻易的达到“搜寻/删除/取代”某特定字串的处理程序!强大的字符串处理能力 3.2、 基础正则表达式3.2.1、 语系对正则表达式的影响&nbsp;&nbsp;计算机文件其实记录的仅有0与1,我们看到的字符文字与数字都是通过编码表转换来的。由于不同语系的编码数据并不相同，所有就会造成数据提取结果的差异了。举例来说，在英文大小写的编码顺序中，zh_TW.big5及C这两种语系的输出结果分别如下: LANG=C时：0 1 2 3 4 … A B C D … Z abcd … z LANG=zh_TW时: 0 1 2 3 4 … a A b B C c … z Z 上面的顺序是编码的顺序，我们可以很清楚的发现这两种语系明显就是不一样！如果你想要提取大写字符而使用[A-Z]时，会发现会发现LANG=C确实可以仅捉到大写字符 (因为是连续的),但是如果 LANG=zh_TW.big5时,就会发现到, 连同小写的b-z也会被撷取出来!因为就编码的顺序来看,big5语系可以撷取到“ A b B c C … z Z ”这一堆字符哩! 所以,使用正则表达式时,需要特别留意当时环境的语系为何, 否则可能会发现与别人不相同的撷取结果喔!一般练习时采用LANG=C这个语系。 一些特殊符号的代表意义 3.2.2、 grep的一些进阶选项 grep是一个很常见也很常用的指令，它最重要的功能就是进行字符串数据的比对，然后将符合使用者需求的字串行印出来。需要说明的是“grep 在数据中查寻一个字串时,是以 “整行” 为单位来进行数据的撷取的!”也就是说,假如一个文件内有 10 行,其中有两行具有你所搜寻的字串,则将那两行显示在屏幕上,其他的就丢弃了! 3.2.3、 基础正则表达式练习练习用文件下载如下：123456789101112131415161718192021222324wget http://linux.vbird.org/linux_basic/0330regularex/regular_express.txt# content\"Open Source\" is a good mechanism to develop programs.apple is my favorite food.Football game is not use feet only.this dress doesn't fit me.However, this dress is about $ 3183 dollars.^MGNU is free air not free beer.^MHer hair is very beauty.^MI can't finish the test.^MOh! The soup taste good.^Mmotorcycle is cheap than car.This window is clear.the symbol '*' is represented as start.Oh! My god!The gd software is a library for drafting programs.^MYou are the best is mean you are the no. 1.The world &lt;Happy&gt; is the same with \"glad\".I like dog.google is the best tools for search keyword.goooooogle yes!go! go! Let's go.# I am VBird 例题一、搜寻特定字串 从上面下载的文件中取the这个特定字串，最简单的方式就是这样：12345678grep -n 'the' regular_express.txt # output8:I can't finish the test.12:the symbol '*' is represented as start.15:You are the best is mean you are the no. 1.16:The world &lt;Happy&gt; is the same with \"glad\".18:google is the best tools for search keyword. 如果要反向选择，也就是当该行没有’the’这个字串时才显示在屏幕上，那就直接使用：1234567891011121314151617181920grep -vn 'the' regular_express.txt # output1:\"Open Source\" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.4:this dress doesn't fit me.5:However, this dress is about $ 3183 dollars.6:GNU is free air not free beer.7:Her hair is very beauty.9:Oh! The soup taste good.10:motorcycle is cheap than car.11:This window is clear.13:Oh! My god!14:The gd software is a library for drafting programs.17:I like dog.19:goooooogle yes!20:go! go! Let's go.21:# I am VBird22: 如果想取得不论大小写的‘the’这个字串，则：12345678910grep -in 'the' regular_express.txt # output8:I can't finish the test.9:Oh! The soup taste good.12:the symbol '*' is represented as start.14:The gd software is a library for drafting programs.15:You are the best is mean you are the no. 1.16:The world &lt;Happy&gt; is the same with \"glad\".18:google is the best tools for search keyword. 例题二、利用中括号[]来搜寻集合字符 如果我想要搜寻test或taste这两个单字时,可以发现到,其实她们有共通的’t?st’存在~这个时候,我可以这样来搜寻:12345grep -n 't[ae]st' regular_express.txt# output8:I can't finish the test.9:Oh! The soup taste good. []这个里面不论有几个字符，都仅匹配某一个字符，，如果想要搜寻到有oo的字符串时，则使用下面的命令： 123456789grep -n 'oo' regular_express.txt # output1:\"Open Source\" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search keyword.19:goooooogle yes! 但是，如果我不想要oo前面有g的话呢？此时，可以利用在集合字符的反向选择来达成：1234567grep -n '[^g]oo' regular_express.txt# output2:apple is my favorite food.3:Football game is not use feet only.18:google is the best tools for search keyword.19:goooooogle yes! 如果我们不想要oo前面有小写字符，可以这样写：1234grep -n '[^a-z]oo' regular_express.txt# output3:Football game is not use feet only. 连续编码可以使用减号-，也可以使用如下的方法来取得前面的两个测试的结果： 123456789grep -n '[^[:lower:]]oo' regular_express.txt# output3:Football game is not use feet only.# 只取含数字的那一行grep -n '[[:digit:]]' regular_express.txt# output5:However, this dress is about $ 3183 dollars.15:You are the best is mean you are the no. 1. 例题三、行首与行尾字符^$ 在上面例子中，可以查询到一行字串里面有the的，那如果我想要让the只在行首列出呢？这个时候就需要使用定位字符了！1234grep -n '^the' regular_express.txt # output12:the symbol '*' is represented as start. 如果我想开头是小写字符的那一行就列出呢？可以这样写：123456789101112grep -n '^[a-z]' regular_express.txt # output2:apple is my favorite food.4:this dress doesn't fit me.10:motorcycle is cheap than car.12:the symbol '*' is represented as start.18:google is the best tools for search keyword.19:goooooogle yes!20:go! go! Let's go.# 也可以使用下面的命令来实现grep -n '^[[:lower:]]' regular_express.txt 如果我不想要开头是英文字母，则可以是这样：1234567grep -n '^[^a-zA-Z]' regular_express.txt # output1:\"Open Source\" is a good mechanism to develop programs.21:# I am VBird# 也可以这样写grep -n '^[^[:alpha:]]' regular_express.txt 注意到了吧?那个^符号,在字符集合符号(括号[])之内与之外是不同的! 在 [] 内代表“反向选择”,在[]之外则代表定位在行首的意义!要分清楚喔! 反过来思考,那如果我想要找出来,行尾结束为小数点 (.) 的那一行,该如何处理:1234567891011121314grep -n '\\.$' regular_express.txt # output1:\"Open Source\" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.4:this dress doesn't fit me.10:motorcycle is cheap than car.11:This window is clear.12:the symbol '*' is represented as start.15:You are the best is mean you are the no. 1.16:The world &lt;Happy&gt; is the same with \"glad\".17:I like dog.18:google is the best tools for search keyword.20:go! go! Let's go. 因为小数点.具有其他意义，所以必须使用转义字符(\\)来加以解除其特殊意义！但是第 5~9 行最后面也是 . 啊~怎么无法打 5~9 印出来? 这里就牵涉到 Windows 平台的软件对于断行字符的判断问题了! 我们使用cat -A前十行的倒数六行。12345678cat -An regular_express.txt| head -n 10 | tail -n 6# output 5 However, this dress is about $ 3183 dollars.^M$ 6 GNU is free air not free beer.^M$ 7 Her hair is very beauty.^M$ 8 I can't finish the test.^M$ 9 Oh! The soup taste good.^M$ 10 motorcycle is cheap than car.$ 在上面的输出中我们可以发现5~9行为Windows的断行字符(^M$),而正常的Linux应该仅有第10行显示的那样($)。所以,那个.自然就不是紧接在$之前喔!也就捉不到5~9 行了!这样可以了解^与$的意义了吧。 如果我想找出哪一行是空白行，也就是该行没有输入任何数据，如何操作？123grep -n '^$' regular_express.txt # output22: 因为只有行首和行尾(^$)，所以这样就可以找出空白行啦。 再来,假设你已经知道在一个程序脚本 (shell script) 或者是配置文件当中,空白行与开头为 # 的那一行是注解,因此如果你要将数据列出给别人参考时,可以将这些数据省略掉以节省保贵的纸张,那么你可以怎么做呢? 我们以 /etc/rsyslog.conf 这个文件来作范例,你可以自行参考一下输出的结果:1234567891011121314151617181920cat -n /etc/rsyslog.conf# 在ubuntu 17.10中，输出了60行，很多空白行与#开头的注解行grep -v '^$' /etc/rsyslog.conf | grep -v -n '^#'# 结果仅有13行，其中第一个`-v '^$'` 代表不要空行# 第二个“-v '^#'”， 代表不要开头是#的那行# output10:module(load=\"imuxsock\") # provides support for local system logging19:module(load=\"imklog\" permitnonkernelfacility=\"on\")27:$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat29:$RepeatedMsgReduction on33:$FileOwner syslog34:$FileGroup adm35:$FileCreateMode 064036:$DirCreateMode 075537:$Umask 002238:$PrivDropToUser syslog39:$PrivDropToGroup syslog43:$WorkDirectory /var/spool/rsyslog47:$IncludeConfig /etc/rsyslog.d/*.conf 例题四、任意一个字符.与重复字符*我们知道通配符*可以用来代表任意（0或多个）字符，但是正则表达式并不是通用字符，两者之间是不同的！至于正则表达式中的.则代表“绝对有一个任意字符”的意思！这两个符号在正则表达式的意义如下： .(小数点)：代表“一定有一个任意字符”的意思 *(星号)：代表“重复前一个字符，0到无穷多次”的意思，为组合形态 这样讲不好懂,我们直接做个练习吧!假设我需要找出 g??d 的字串,亦即共有四个字符,起头是 g 而结束是 d ,我可以这样做:12345grep -n 'g..d' regular_express.txt # output1:\"Open Source\" is a good mechanism to develop programs.9:Oh! The soup taste good.16:The world &lt;Happy&gt; is the same with \"glad\". 再来,如果我想要列出有 oo, ooo, oooo 等等的数据, 也就是说,至少要有两个(含)o以上,该如何是好?是 o 还是 oo 还是 ooo* 呢? 虽然你可以试看看结果, 不过结果太占版面了。 因为*代表的是重复 0 个或多个前面的 RE 字符”的意义, 因此,o*代表的是具有空字符或一个o以上的字符, 特别注意,因为允许空字符(就是有没有字符都可以的意思),因此,grep -n &#39;o*&#39; regular_express.txt将会把所有的数据都打印出来屏幕上! 那如果是oo*呢?则第一个o肯定必须要存在,第二个o则是可有可无的多个o, 所以,凡是含有o,oo,ooo,oooo等等,都可以被列出来~ 同理,当我们需要至少两个o以上的字串时,就需要ooo*,亦即是:12345678grep -n 'ooo*' regular_express.txt # output1:\"Open Source\" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search keyword.19:goooooogle yes! 如果我们想要字符串开头和结尾都是g,但是两个g之间仅能存在至少一个o，亦即是gog,goog,gooog..等等，那该如何？123grep -n 'g.*g' regular_express.txt # . 代表任意一个字符# .*代表任意个字符 如果我想找出含’任意数字‘的行列呢，所以就成为：1234grep -n '[0-9][0-9]*' regular_express.txt # output5:However, this dress is about $ 3183 dollars.15:You are the best is mean you are the no. 1. 例题五、限定连续RE字符范围 在上个例题当中,我们可以利用 . 与 RE 字符及 * 来设置 0 个到无限多个重复字符, 那如果我想要限制一个范围区间内的重复字符数呢?举例来说,我想要找出两个到五个 o 的连续字串,该如何做?这时候就得要使用到限定范围的字符 {} 了。 但因为 { 与 } 的符号在 shell 是有特殊意义的,因此, 我们必须要使用转义字符 \\ 来让他失去特殊意义才行。 至于 {} 的语法是这样的,假设我要找到两个 o 的字串,可以是:1234567891011121314grep -n 'o\\&#123;2\\&#125;' regular_express.txt # output1:\"Open Source\" is a good mechanism to develop programs.2:apple is my favorite food.3:Football game is not use feet only.9:Oh! The soup taste good.18:google is the best tools for search keyword.19:goooooogle yes!grep -n 'go\\&#123;2,5\\&#125;g' regular_express.txt # output18:google is the best tools for search keyword.# 没有了第19行，因为它有6个o 3.2.4、 基础正则表达式字符汇整(characters)经过上面的几个简单的练习，我们可以将基础的正则表达式特殊字符汇整如下表： 正则表达式的特殊字符”与一般在命令行输入指令的“万用字符”并不相同。 例如。在万用字符当中的 代表的是“ 0 ~ 无限多个字符”的意思,但是在正则表达式当中, 则是“重复0到无穷多个的前一个 RE 字符”的意思~使用的意义并不相同,不要搞混了! 举例来说,不支持正则表达式的 ls 这个工具中,若我们使用 “ls -l ” 代表的是任意文件名的文件,而 “ls -l a ”代表的是以 a 为开头的任何文件名的文件, 但在正则表达式中,我们要找到含有以 a 为开头的文件,则必须要这样:(需搭配支持正则表达式的工具) ls | grep -n &apos;^a.*&apos; 要想以ls -l配合grep找出/etc/下面文件类型为链接文件属性的文件名，该如何做呢？由于ls -l列出链接文件时标头会是lrwxrwxrwx,因此使用如下的指令即可找出结果： ls -l /etc \\grep &apos;^l&apos; # 若仅想列出几个文件，再以`| wc -l`来累加处理即可。 3.2.5、 sed工具在了解了一些正则表达式的基础应用之后,再来呢?呵呵~两个东西可以玩一玩的,那就是sed 跟下面会介绍的 awk 了! 这两个家伙可是相当的有用的啊! 我们先来谈一谈 sed 好了, sed 本身也是一个管线命令,可以分析 standard input 的啦! 而且 sed 还可以将数据进行取代、删除、新增、撷取特定行等等的功能呢!很不错吧~ 我们先来了解一下 sed 的用法,再来聊他的用途好了! 以行为单位的新增/删除功能 sed光是用看的，是看不懂的啦！，又要开始练习了 范例一：将/etc/passwd的内容拷贝pd.txt列出并且打印行号，同时，请将第2-5行删除！1234567nl pd.txt | sed '2,5d'# output 1 root:x:0:0:root:/root:/bin/bash 6 games:x:5:60:games:/usr/games:/usr/sbin/nologin 7 man:x:6:12:man:/var/cache/man:/usr/sbin/nologin 8 lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin ...........后面省略................ 看到了吧?sed 的动作为 ‘2,5d’ ,那个 d 就是删除!因为 2-5 行给他删除了,所以显示的数据就没有 2-5 行啰~ 另外,注意一下,原本应该是要下达 sed -e 才对,没有 -e 也行啦!同时也要注意的是, sed 后面接的动作,请务必以 ‘’ 两个单引号括住喔! 如果只删除第2行，可以使用nl pd.txt | sed &#39;2d&#39;来实现，若要删除第3到最后一行，则可以使用nl pd.txt |sed &#39;3,$d&#39;,其中$代表最后一行！ 范例二：承上题，在第二行后（亦即是加在第三行）加上“drink tea?”字样！123456nl pd.txt | sed '2a drink tea?'# output 1 root:x:0:0:root:/root:/bin/bash 2 daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologindrink tea? 3 bin:x:2:2:bin:/bin:/usr/sbin/nologin 嘿嘿!在 a 后面加上的字串就已将出现在第二行后面啰!那如果是要在第二行前呢?nl pd.txt | sed &#39;2i drink tea&#39;就对啦!就是将“ a ”变成“ i ”即可。 增加一行很简单,那如果是要增将两行以上呢? 范例三：在第二行后面加入两行字，例如“drink tea or …”与”drink beer”字样！123456789nl pd.txt | sed '2a drink tea or ... \\ &gt; drink beer?'# output 1 root:x:0:0:root:/root:/bin/bash 2 daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologindrink tea or ... drink beer? 3 bin:x:2:2:bin:/bin:/usr/sbin/nologin.............后面省略........ 这个范例的重点是“我们可以新增不只一行喔!可以新增好几行”但是每一行之间都必须要以反斜线“ \\ ”来进行新行的增加喔!所以,上面的例子中,我们可以发现在第一行的最后面就有 \\存在啦!在多行新增的情况下, \\ 是一定要的喔! 以行为单位的取代与显示功能 刚刚只是介绍了如何新增与删除，那么如果要整行取代呢？ 范例四：我想将第2-5行的内容取代成为”No 2-5 number”呢？12345nl pd.txt | sed '2,5c No 2-5 number'# output 1 root:x:0:0:root:/root:/bin/bashNo 2-5 number 6 games:x:5:60:games:/usr/games:/usr/sbin/nologin 通过这个方法我们就能够将数据整行取代了!非常容易吧!sed 还有更好用的东东!我们以前想要列出第 11~20 行, 得要通过head -n 20 | tail -n 10之类的方法来处理,很麻烦啦~ sed则可以简单的直接取出你想要的那几行!是通过行号来捉的喔!看看下面的范例先: 范例五：仅列出pd.txt文件内的第5-7行12345nl pd.txt | sed -n '5,7p'# output 5 sync:x:4:65534:sync:/bin:/bin/sync 6 games:x:5:60:games:/usr/games:/usr/sbin/nologin 7 man:x:6:12:man:/var/cache/man:/usr/sbin/nologin 上述的指令中有个重要的选项-n,按照说明文档,这个 -n 代表的是“安静模式”! 那么为什 么要使用安静模式呢?你可以自行下达sed ‘5,7p’` 就知道了 (5-7 行会重复输出)! 有没有加上 -n 的参数时,输出的数据可是差很多的喔!你可以通过这个 sed 的以行为单位的显示功能, 就能够将某一个文件内的某些行号捉出来查阅!很棒的功能!不是吗? 部分数据的搜寻并取代的功能 除了整行的处理模式之外，sed还可以用行为单位进行部分数据的查找并替换的功能，基本上sed的查找与替换与vi类似！ sed &apos;s/要被取代的字串/新的字串/g&apos; 上表中特殊字体的部分为关键字,请记下来!至于三个斜线分成两栏就是新旧字串的替换啦。我们使用下面这个取得IP数据的范例,一段一段的来处理,让你了解一下什么是咱们所谓的查找并替换吧!1234567891011121314151617181920212223# step1 : 先观察源信息，利用`sbin/ifconfig` 查询IP /sbin/ifconfig enp3s0 # output enp3s0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 219.223.243.131 netmask 255.255.248.0 broadcast 219.223.247.255 inet6 fe80::1234:4cee:444b:1d66 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 2001:250:3c02:301:c9a3:6a1d:d5ce:ec5f prefixlen 128 scopeid 0x0&lt;global&gt; # 重点在第二行，也就是ip地址那一行# step2 :利用关键字配合grep选取出关键的一行数据 ifconfig enp3s0 | grep 'inet ' # output inet 219.223.243.131 netmask 255.255.248.0 broadcast 219.223.247.255# step3：将IP前面的部分予以删除 ifconfig enp3s0 | grep 'inet ' | sed 's/^.*inet //g' # output 219.223.243.131 netmask 255.255.248.0 broadcast 219.223.247.255# step4：将IP后面的部分删除 ifconfig enp3s0 | grep 'inet ' | sed 's/^.*inet//g' | sed 's/ netmask.*$//g' # output 219.223.243.131 通过这个范例的练习也建议您依据此一步骤来研究你的指令!就是先观察,然后再一层一层的试做, 如果有做不对的地方,就先予以修改,改完之后测试,成功后再往下继续测试。 直接修改文件内容（危险动作） 你以为 sed 只有这样的能耐吗?那可不! sed 甚至可以直接修改文件的内容呢!而不必使用管线命令或数据流重导向! 不过,由于这个动作会直接修改到原始的文件,所以请你千万不要随便拿系统配置文件来测试喔! 我们还是使用你下载的 regular_express.txt 文件来测试看看吧! 范例六：利用sed将regular_express.txt内每一行结尾若为.则换成！。123456789101112131415161718192021222324sed -i 's/\\.$/\\!/g' regular_express.txt cat regular_express.txt # output\"Open Source\" is a good mechanism to develop programs!apple is my favorite food!Football game is not use feet only!this dress doesn't fit me!However, this dress is about $ 3183 dollars.GNU is free air not free beer.Her hair is very beauty.I can't finish the test.Oh! The soup taste good.motorcycle is cheap than car!This window is clear!the symbol '*' is represented as start!Oh! My god!The gd software is a library for drafting programs.You are the best is mean you are the no. 1!The world &lt;Happy&gt; is the same with \"glad\"!I like dog!google is the best tools for search keyword!goooooogle yes!go! go! Let's go!# I am VBird 范例七：利用sed直接在regular_express.txt最后一行加入# This is a test1234567sed -i '$a # This is a test' regular_express.txtnl regular_express.txt| sed -n '20,25p'# output 20 go! go! Let's go! 21 # I am VBird 22 # This is a test sed 的“ -i ”选项可以直接修改文件内容,这功能非常有帮助!举例来说,如果你有一个 100 万行的文件,你要在第 100 行加某些文字,此时使用 vim 可能会疯掉!因为文件太大了!那怎办?就利用 sed 啊!通过 sed 直接修改/取代的功能,你甚至不需要使用 vim 去修订!很棒吧! 3.3、 扩展正则表达式事实上,一般读者只要了解基础型的正则表达式大概就已经相当足够了,不过,某些时刻为了要简化整个指令操作, 了解一下使用范围更广的延伸型正则表达式的表示式会更方便呢!举个简单的例子好了,在上节的例题三的最后一个例子中,我们要去除空白行与行首为 # 的行列,使用的是 grep -v &apos;^$&apos; regular_express.txt | grep -v &apos;^#&apos; 需要使用到管线命令来搜寻两次！那么如果使用延伸型的正则表达式，我们可以简化为： egrep -v &apos;^$|^#&apos; regular_express.txt 延伸型正则表达式可以通过群组功能“ | ”来进行一次搜寻!那个在单引号内的管线意义为“或or”啦! 是否变的更简单呢?此外,grep 默认仅支持基础正则表达式,如果要使用延伸型正则表达式,你可以使用 grep -E , 不过更建议直接使用 egrep !直接区分指令比较好记忆!其实 egrep 与 grep -E 是类似命令别名的关系。 熟悉了正则表达式之后,到这个延伸型的正则表达式,你应该也会想到,不就是多几个重要的特殊符号吗? 是的~所以,我们就直接来说明一下,延伸型正则表达式的特殊符如下表所示： 例子测试结果如下图所示： !不是特殊字符，如果想要查出来文件中含有！与&gt;的字行时，可以这样操作： grep -n ‘[!&gt;]’ regular_express.txt 3.4、 文件的格式化与相关处理接下来让我们来将文件进行一些简单的编排吧!下面这些动作可以将你的讯息进行排版的动作, 不需要重新以 vim 去编辑,通过数据流重导向配合下面介绍的 printf 功能,以及 awk 指令, 就可以让你的讯息以你想要的模样来输出了!试看看吧! 3.4.1、 格式化打印：printf 3.4.2、 awk：好用的数据处理工具awk 也是一个非常棒的数据处理工具!相较于 sed 常常作用于一整个行的处理, awk 则比较倾向于一行当中分成数个“字段”来处理。因此,awk 相当的适合处理小型的数据数据处理!awk 通常运行的模式是这样的: awk &apos;条件类型1{动作1} 条件类型2{动作2} .... &apos; filename awk 后面接两个单引号并加上大括号 {} 来设置想要对数据进行的处理动作。 awk 可以处理后续接的文件,也可以读取来自前个指令的 standard output 。 但如前面说的, awk 主要是处理“每一行的字段内的数据”,而默认的“字段的分隔符号为 “空白键” 或 “[tab]键” ”!举例来说,我们用 last 可以将登陆者的数据取出来,结果如下所示:（下面命令是在我购买的远程主机上进行的） 1234567last -n 5# outputroot pts/1 58.60.1.102 Tue Apr 10 22:29 still logged in root pts/0 58.60.1.76 Tue Apr 10 22:21 still logged in root pts/0 58.60.1.76 Sun Apr 8 20:58 - 21:36 (00:37) root pts/0 58.60.1.76 Fri Mar 23 08:19 - 08:19 (00:00) reboot system boot 4.13.9-1.el6.elr Tue Jan 23 22:26 - 22:30 (76+23:03) 若我想取出帐号与登录者的IP，且帐号与IP之间一[Tab]隔开，可以进行如下操作：1234567last -n 5 | awk '&#123;print $1 \"\\t\" $3&#125;'# outputroot 58.60.1.102root 58.60.1.76root 58.60.1.76root 58.60.1.76reboot boot 上表是 awk 最常使用的动作!通过 print 的功能将字段数据列出来!字段的分隔则以空白键或[tab] 按键来隔开。 因为不论哪一行我都要处理,因此,就不需要有 “条件类型” 的限制!我所想要的是第一栏以及第三栏, 但是,第五行的内容怪怪的~这是因为数据格式的问题啊!所以~使用 awk 的时候,请先确认一下你的数据当中,如果是连续性的数据,请不要有空格或 [tab]在内,否则,就会像这个例子这样,会发生误判! 另外,由上面这个例子你也会知道,在awk的括号内,每一行的每个字段都是有变量名称的,那就是 $1, $2… 等变量名称。以上面的例子来说, root是$1,因为他是第一栏!至于58.60.1.102是第三栏, 所以他就是$3!后面以此类推~呵呵!还有个变量!那就是 $0 ,$0 代表“一整列数据”的意思~以上面的例子来说,第一行的 $0 代表的就是一整行数据的意思。 由此可知,刚刚上面五行当中,整个 awk 的处理流程是:1234step1：读入第一行，并将第一行的数据填入 $0, $1, $2.... 等变量当中;step2：依据”条件类型“的限制，判断是否需要进行后面的”动作“;step3：做完所有的动作与条件类型step4：若还有后续的‘行’的数据，则重复上面1-3的步骤，知道所有的数据都读完为止。 经过这样的步骤,你会晓得, awk 是“以行为一次处理的单位”, 而“以字段为最小的处理单位”。好了,那么 awk 怎么知道我到底这个数据有几行?有几栏呢?这就需要 awk 的内置变量的帮忙啦~ 变量名称 代表意义 NF 每一行($0)拥有的字段总数 NR 目前awk所处理的是“第几行”数据 FS 目前的分隔字符，默认是空白键 继续上面的例子，如果我想实现以下效果： &emsp;列出每一行的帐号(就是 $1); &emsp;列出目前处理的行数(就是 awk 内的 NR 变量) &emsp;并且说明,该行有多少字段(就是 awk 内的 NF 变量) 1234567last -n 5 | awk '&#123;print $1 \"\\t lines: \" NR \"\\t columns: \" NF&#125;'# outputroot lines: 1 columns: 10root lines: 2 columns: 10root lines: 3 columns: 10root lines: 4 columns: 10reboot lines: 5 columns: 11 awk的逻辑运算字符 既然有需要用到“条件”的类别，自然就需要一些逻辑运算咯～例如下面这些，如下表所示： 运算单元 代表意义 &gt; 大于 &lt; 小于 &gt;= 大于等于 &lt;= 小于等于 == 等于 != 不等于 好了,我们实际来运用一下逻辑判断吧!举例来说,在 pd.txt 当中是以冒号 “:” 来作为字段的分隔, 该文件中第一字段为帐号,第三字段则是 UID。那假设我要查阅,第三栏小于10 以下的数据,并且仅列出帐号与第三栏, 那么可以这样做:12345678910111213cat pd.txt | \\&gt; awk '&#123;FS=\":\"&#125; $3 &lt; 10 &#123;print $1 \"\\t\" $3&#125;'# outputroot:x:0:0:root:/root:/bin/bash daemon 1bin 2sys 3sync 4games 5man 6lp 7mail 8news 9 有趣吧!不过,怎么第一行没有正确的显示出来呢?这是因为我们读入第一行的时候,那些变量 $1, $2… 默认还是以空白键为分隔的,所以虽然我们定义了 FS=”:” 了, 但是却仅能在第二行后才开始生效。那么怎么办呢?我们可以预先设置 awk 的变量啊! 利用 BEGIN 这个关键字喔!这样做:123456789cat pd.txt | \\&gt; awk 'BEGIN &#123;FS=\":\"&#125; $3 &lt; 10 &#123;print $1 \"\\t\" $3&#125;'# outputroot 0daemon 1bin 2sys 3sync 4games 5 我有一个薪资数据表文件名为pay.txt，内容是这样的：1234Name 1st 2nd 3thVd 23000 24000 25000Di 21000 20000 23000Bd 43000 42000 41000 如果我想计算每个人的总额呢？而且我还想要格式化输出，我们可以这样考虑： 第一行只是说明，所以第一行不要进行加总（NR==1时处理） 第二行以后就会有加总的情况出现（NR &gt;=2 以后处理 ） 123456789cat pay.txt | \\&gt; awk 'NR == 1 &#123;printf \"%10s %10s %10s %10s %10s\\n\",$1,$2,$3,$4,\"Total\"&#125;&gt; NR &gt;= 2 &#123;Total = $2+$3+$4&gt; printf \"%10s %10s %10s %10s %10.2f\\n\",$1,$2,$3,$4,Total&#125;'# output Name 1st 2nd 3th Total Vd 23000 24000 25000 72000.00 Di 21000 20000 23000 64000.00 Bd 43000 42000 41000 126000.00 上面的例子有几个重要事项应该先说明的： awk的指令间隔:所有awk的动作,亦即在{}内的动作,如果有需要多个指令辅助时,可利用分号“;”间隔, 或者直接以 [Enter] 按键来隔开每个指令,例如上面的范例中我就使用了三次 [enter] 喔! 逻辑运算当中，如果是等于的情况，则务必使用两个等号“==”！ 格式化输出时，在printf的格式设置当中，务必加上\\n，才能进行分行 与bash shell的变量不同，在awk当中，变量可以直接使用，不需加上$符号 利用 awk 这个玩意儿,就可以帮我们处理很多日常工作了呢!真是好用的很~ 此外, awk的输出格式当中,常常会以 printf 来辅助,所以, 最好你对 printf 也稍微熟悉一下比较好啦!另外, awk 的动作内 {} 也是支持 if (条件) 的喔! 举例来说,上面的指令可以修改成为这样:123456789cat pay.txt | \\&gt; awk '&#123;if (NR==1) printf \"%10s %10s %10s %10s %10s\\n\",$1,$2,$3,$4,\"Total\"&#125;&gt; NR &gt;= 2 &#123;Total = $2+$3+$4&gt; printf \"%10s %10s %10s %10s %10.2f\\n\",$1,$2,$3,$4,Total&#125;'# output Name 1st 2nd 3th Total Vd 23000 24000 25000 72000.00 Di 21000 20000 23000 64000.00 Bd 43000 42000 41000 126000.00 3.4.3、 文件比较工具什么时候会用到文件的比对啊?通常是同一个软件的不同版本之间,比较配置文件与原始文件的差别。很多时候所谓的文件比对,通常是用在ASCII纯文本文件的比对上的!那么比对文件的指令有哪些?最常见的就是diff啰!另外,除了diff比对之外,我们还可以借由cmp来比对非纯文本文件!同时,也能够借由 diff 创建的分析档,以处理补丁(patch)功能的文件呢!就来玩玩先! diff diff就是用在比对两个文件之间的差异的,并且是以行为单位来比对的!一般是用在ASCII纯文本文件的比对上。由于是以行为比对的单位,因此diff通常是用在同一的文件(或软件)的新旧版本区别上! 举例来说,假如我们要将/etc/passwd处理成为一个新的版本pd,处理方式为:将第四行删除,第六行则取代成为“no six line”,新的文件放置到 /shell_learning/test 里面,那么应该怎么做? 123456789101112cd ~/Desktop/shell_learningmkdir testcp /etc/passwd pdcat pd | sed -e '4d' -e '6c no six line' &gt; pd.new# outputcat -n pd.new | head -n 6 1 root:x:0:0:root:/root:/bin/bash 2 daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin 3 bin:x:2:2:bin:/bin:/usr/sbin/nologin 4 sync:x:4:65534:sync:/bin:/bin/sync 5 no six line 6 man:x:6:12:man:/var/cache/man:/usr/sbin/nologin 接下来讨论一下关于diff的用法吧！12345678diff [-bBi] from-file to-file# paras： from-file : 一个文件名,作为原始比对文件的文件名; to-file : 一个文件名,作为目的比对文件的文件名; 注意,from-file 或 to-file 可以 - 取代,那个 - 代表“Standard input”之意。 -b :忽略一行当中,仅有多个空白的差异(例如 \"about me\" 与 \"about me\" 视为相同 -B :忽略空白行的差异。 -i :忽略大小写的不同。 范例一:比对pd与pd.new的差异12345678diff pd pd.new # output4d3 &lt;==左边第四行被删除(d)了,基准是右边第三行&lt; sys:x:3:3:sys:/dev:/usr/sbin/nologin &lt;===这边列出左边(&lt;)文件被删除的那一行内容6c5 &lt;==左边文件的第六行被替换(c)成右边文件的第五行&lt; games:x:5:60:games:/usr/games:/usr/sbin/nologin &lt;===左边(&lt;)文件的第六行内容---&gt; no six line &lt;==右边(&gt;)文件第五行内容 很聪明吧！用diff把我们刚刚的处理给比对完毕了！ diff也可以比较整个目录下的区别 diff还可以比较不同目录下的相同文件名的内容 cmp 相对于diff的广泛用途,cmp似乎就用的没有这么多了~cmp主要也是在比对两个文件,他主要利用“字节”单位去比对, 因此,当然也可以比对binary file啰~(还是要再提醒喔,diff主要是以“行”为单位比对,cmp则是以“字节”为单位去比对,这并不相同!)1234567cmp [-l] file1 file2# paras: -l :将所有的不同点的字节处都列出来。因为 cmp 默认仅会输出第一个发现的不同点。# examples: cmp pd pd.new # output pd pd.new differ: byte 120, line 4 patch patch 这个指令与 diff 可是有密不可分的关系啊!我们前面提到,diff 可以用来分辨两个版本之间的差异, 举例来说,刚刚我们所创建的pd及pd.new之间就是两个不同版本的文件。那么,如果要“升级”呢?就是“将旧的文件升级成为新的文件”时,应该要怎么做呢?其实也不难啦!就是“先比较先旧版本的差异,并将差异档制作成为补丁文件,再由补丁文件更新旧文件”即可。举例来说,我们可以这样做测试: 范例一：以pd与pd.new制作补丁文件1234567891011121314151617diff -Naur pd pd.new &gt; pd.patchcat pd.patch# output--- pd 2018-04-11 13:03:58.165695684 +0800 &lt;== 新旧文件的信息+++ pd.new 2018-04-11 13:08:07.710232163 +0800@@ -1,9 +1,8 @@ &lt;== 新旧文件要修改数据的界定范围，旧文件1-9行，新文件1-8行 root:x:0:0:root:/root:/bin/bash daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin bin:x:2:2:bin:/bin:/usr/sbin/nologin-sys:x:3:3:sys:/dev:/usr/sbin/nologin &lt;== 左侧文件被删除 sync:x:4:65534:sync:/bin:/bin/sync &lt;== 左侧文件被删除-games:x:5:60:games:/usr/games:/usr/sbin/nologin+no six line &lt;== 右侧新文件加入 man:x:6:12:man:/var/cache/man:/usr/sbin/nologin lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin mail:x:8:8:mail:/var/mail:/usr/sbin/nologin 一般来说,使用diff制作出来的比较文件通常使用扩展名为.patch啰。至于内容就如同上面介绍的样子。基本上就是以行为单位,看看哪边有一样与不一样的,找到一样的地方,然后将不一样的地方取代掉! 以上面表格为例,新文件看到-会删除,看到+会加入!好了,那么如何将旧的文件更新成为新的内容呢? 就是将pd改成与pd.new相同!可以这样做:12345patch -pN &lt; patch_file &lt;==更新patch -R -pN &lt; patch_file &lt;==还原# paras:-p :后面可以接“取消几层目录”的意思。-R :代表还原,将新的文件还原成原来旧的版本。 范例二：将刚才制作出来的patch file用来更新旧版数据123456789patch -p0 &lt; pd.patch# outputpatching file pdheany@heany:~/Desktop/shell-learning$ ll pd*-rw-r--r-- 1 heany heany 2253 4月 11 13:43 pd-rw-r--r-- 1 heany heany 2253 4月 11 13:08 pd.new## 两个文件大小一模一样 范例三：将刚才制作出来的patch file恢复旧文件的内容123456789patch -R -p0 &lt; pd.patch# outputpatching file pdheany@heany:~/Desktop/shell-learning$ ll pd*-rw-r--r-- 1 heany heany 2326 4月 11 13:47 pd-rw-r--r-- 1 heany heany 2253 4月 11 13:08 pd.new## 文件恢复成旧的版本了 为什么这里会使用-p0呢?因为我们在比对新旧版的数据时是在同一个目录下,因此不需要减去目录啦!如果是使用整体目录比对(diff旧目录 新目录)时, 就得要依据创建patch文件件所在目录来进行目录的删减啰! 3.4.4、 文件打印准备：pr如果你曾经使用过一些图形接口的文书处理软件的话,那么很容易发现,当我们在打印的时候, 可以同时选择与设置每一页打印时的标头吧!也可以设置页码呢!那么,如果我是在Linux 下面打印纯文本文件呢 可不可以具有标题啊?可不可以加入页码啊?呵呵!当然可以啊!使用 pr 就能够达到这个功能了。不过, pr 的参数实在太多了,说不完,一般来说,仅使用最简单的方式来处理就行。举例来说,如果想要打印pd呢?12345678heany@heany:~/Desktop/shell-learning$ pr pd2018-04-11 13:47 pd Page 1root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologin....................下面省略.......... 四、 学习Shell Scripts4.1、什么是shell scriptshell script可以简单的被看成是批处理文件,也可以被说成是一个程序语言,且这个程序语言由于都是利用shell与相关工具指令, 所以不需要编译即可执行,且拥有不错的除错(debug)工具,所以,他可以帮助系统管理员快速的管理好主机。 shell script其实就是纯文本文件,我们可以编辑这个文件,然后让这个文件来帮我们一次执行多个指令,或者是利用一些运算与逻辑判断来帮我们达成某些功能。所以啦,要编辑这个文件的内容时,当然就需要具备有 bash 指令下达的相关认识。下达指令需要注意的事项在第四章的开始下达指令小节内已经提过,有疑问请自行回去翻阅。 在shell script的撰写中还需要用到下面的注意事项: 1. 指令的执行是从上而下、从左到右的分析与执行; 2. 指令的下达：指令、选项与参数间的多个空白都会被忽略掉; 3. 空白行也将被忽略掉,并且 [tab] 按键所得的空白同样视为空格键; 4. 如果读取到一个Enter符号（CR），就尝试开始执行该行（该串）命令; 5. 至于如果一行的内容太多，则可以使用&quot;[Enter]&quot;来延伸至下一行; 6. “ # ”可做为注解!任何加在 # 后面的数据将全部被视为注解文字而被忽略! 如此一来,我们在script内所撰写的程序,就会被一行一行的执行。现在我们假设你写的这个程序文件名是/home/heany/Desktop/shell-learning/shell.sh好了,那如何执行这个文件?很简单,可以有下面几个方法: 直接下达指令：shell.sh文件必须要具备可读与可执行（rx）的权限，然后： 绝对路径：使用/home/heany/Desktop/shell-learning/shell.sh来下达指令; 相对路径：假设工作目录在/home/heany/Desktop/shell-learning，则使用./shell.sh来执行; 变量PATH功能：将shell.sh放在PATH指定的目录内，例如：~/bin/ 以bash程序来执行：通过”bash shell.sh”或sh shell.sh来执行。 编写第一个script123456789#!/bin/bash# Program:# This program shows \"Hello World!\" in your screen.# History:# 2018/04/11 VBird First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHecho -e \"Hello World! \\a \\n\"exit 0 程序说明如下： 第一行#!/bin/bash在宣告这个script使用的shell名称: 因为我们使用的是bash ,所以,必须要以#!/bin/bash来宣告这个文件内的语法使用bash语法!那么当这个程序被执行时,他就能够载入bash的相关环境配置文件 (一般来说就是non-login shell的 ~/.bashrc), 并且执行bash来使我们下面的指令能够执行!这很重要的!(在很多状况中,如果没有设置好这一行,那么该程序很可能会无法执行,因为系统可能无法判断该程序需要使用什么 shell 来执行啊!) 程序内容的说明: 整个 script 当中,除了第一行的#!是用来宣告shell的之外,其他的#都是“注解”用途! 所以上面的程序当中,第二行以下就是用来说明整个程序的基本数据。一般来说, 建议你一定要养成说明该 script 的:1. 内容与功能; 2. 版本信息; 3.作者与联络方式; 4. 创建日期;5. 历史纪录 等等。这将有助于未来程序的改写与 debug呢! 主要环境变量的声明: 建议务必要将一些重要的环境变量设置好,个人认为,PATH与LANG (如果有使用到输出相关的信息时) 是当中最重要的! 如此一来,则可让我们这支程序在进行时,可以直接下达一些外部指令,而不必写绝对路径呢!比较方便啦！ 主要程序部分就将主要的程序写好即可！在这个例子当中，就是echo那一行。 告知执行结果：一个命令的执行成功与否，可以使用$?这个变量来观察～那么我们也可以利用exit这个指令来让程序中断，并且回传一个数值给系统。在我们这个例子当中,我使用exit 0,这代表离开script并且回传一个0给系统, 所以我执行完这个script后,若接着下达echo $? 则可得到0的值喔! 更聪明的读者应该也知道了!利用这个exit n(n 是数字) 的功能,我们还可以自订错误讯息, 让这支程序变得更加的smart呢! 1234chmod a+x hello.sh./hello.sh# outputHello World! 编写shell script的良好习惯 一个良好习惯的养成是很重要的~大家在刚开始撰写程序的时候,最容易忽略这部分, 认为程序写出来就好了,其他的不重要。其实,如果程序的说明能够更清楚,那么对你自己是有很大的帮助的。要学会比较仔细的将程序的设计过程给它记录下来，而且记录一些历史记录。在每个script的文件开始处记录好： 1. `script`的功能 2. `script`的版本信息 3. `script`的作者与联系方式 4. `script`的版权声明方式 5. `script`的`History`（历史记录） 6. `script`内比较特殊的指令，使用“绝对路径”的方式下达 7. `script`运行时需要的环境变量预先宣告与设置 8. 在较为特殊程序代码部分，加上适当的注释说明 程序代码的编写最好使用嵌套方式，最好能以[tab]按键的空格缩排这样代码看起来非常漂亮与有条理。 4.2、 简单的shell练习4.2.1、 简单范例 交互式脚本：变量内容由用户决定 请你以read命令的用途,编写一个script,他可以让使用者输入first name与last name,最后并且在屏幕上显示:“Your full name is: ”的内容:123456789101112131415161718192021# showname.sh#!/bin/bash# paragram:# User inputs his first name and last name. Program shows his full name.# History# 2018/04/11 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATH# 提示使用者输入read -p \"Please input your first name: \" firstname# 提示使用者输入read -p \"Please input your last name: \" lastname# 结果由屏幕输出echo -e \"\\nYour full name is: $&#123;firstname&#125; $&#123;lastname&#125;\" ## 执行脚本 . showname.shPlease input your first name: heanyPlease input your last name: boyYour full name is: heany boy 随日期变化：利用data进行文件的创建 想像一个状况,假设我的服务器内有数据库,数据库每天的数据都不太一样,因此当我备份时,希望将每天的数据都备份成不同的文件名,这样才能够让旧的数据也能够保存下来不被覆盖。哇!不同文件名呢!这真困扰啊?难道要我每天去修改script? 不需要啊!考虑每天的“日期”并不相同,所以我可以将文件名取成类似:backup.2015-07-16.data, 不就可以每天一个不同文件名了吗?呵呵!确实如此。那个2015-07-16怎么来的?那就是重点啦!接下来出个相关的例子: 假设我想要创建三个空的文件 (通过 touch)文件名最开头由使用者输入决定,假设使用者输入filename好了,那今天的日期是2015/07/16, 我想要以前天、昨天、今天的日期来创建这些文件,亦即filename_20150714,filename_20150715, filename_20150716 ,该如何是好?123456789101112131415161718192021#!/bin/bash# program:# Program creates three files, which named by user's input and date command.# History:# 2018/04/11 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATH# 1. 让使用者输入文件名称,并取得 fileuser 这个变量;echo -e \"I will use 'touch' command to create 3 files.\" # 纯粹显示信息read -p \"Please input your filename: \" fileuser # 提示使用者输入# 2. 为了避免使用者随意按Enter,利用变量功能分析文件名是否有设置filename=$&#123;fileuser:-\"filename\"&#125; # 开始判断有否配置文件名# 3. 开始利用 date 指令来取得所需要的文件名了;date1=$(date --date='2 days ago' +%Y%m%d) # 前两天的日期date2=$(date --date='1 days ago' +%Y%m%d) # 前一天的日期date3=$(date +%Y%m%d) # 今天的日期file1=$&#123;filename&#125;$&#123;date1&#125; # 下面三行在配置文件名file2=$&#123;filename&#125;$&#123;date2&#125;file3=$&#123;filename&#125;$&#123;date3&#125; 在一串命令中如果还需要通过其他的命令提供的信息，可以使用反单引号`命令`或$(命令)。filename=${fileuser:-&quot;filename&quot;}，这条命令用来判断fileuser是否已经赋值，:-是一起的，fileuser如果有值的话，就用所拥有的值赋给filename变量，如果没有值的话，就把”filename”赋给fileuser，再赋给filename变量。 123456789v=$(uname -r)echo $v# output4.13.0-38-genericvv=`uname -r`echo $vv# output4.13.0-38-generic 数值运算：简单的加减乘除 我们可以使用declare来定义变量的类型吧? 当变量定义成为整数后才能够进行加减运算啊!此外,我们也可以利用$((计算式))来进行数值运算的。可惜的是,bash shell里头默认仅支持到整数的数据而已。OK!那我们来玩玩看,如果我们要使用者输入两个变量, 然后将两个变量的内容相乘,最后输出相乘的结果,那可以怎么做?1234567891011121314#!/bin/bash# Program:# User inputs 2 integer numbers; program will cross thest two numbers.# History:# 2018/04/11 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHecho -e \"You SHOULD input 2 numbers, I will multiplying them! \\n\"read -p \"first number: \" firstnuread -p \"second number: \" secnutotal=$(($&#123;firstnu&#125;*$&#123;secnu&#125;))echo -e \"\\nThe result of $firstnu x $secnu is ==&gt; $total\" 在数值计算上，我们还可以使用declare -i total=$fistnu*$secnu,不推荐使用这种方式，因为比较繁琐，不容易记。使用上面程序中的那种方式来计算数值。 如果想要计算含有小数点的数据时，其实可以通过bc这个指令的协助，例如： echo &quot;123.123*55.9&quot; | bc # output 6882.575 4.3、 善用判断式$?这个变量代表的是上一条命令执行的情况，若没问题则回传码为0,若存在问题，回传错误代码(非0), 此外,也通过&amp;&amp;及||来作为前一个指令执行回传值对于后一个指令是否要进行的依据。我们知道想要判断一个目录是否存在, 之前我们使用的是ls这个指令搭配数据流重导向,最后配合$?来决定后续的指令进行与否。但是否有更简单的方式可以来进行“条件判断”呢?有的~那就是test这个指令。 4.3.1、 利用test指令的测试功能当我们要检测系统上面某些文件或者是相关的属性时，利用test这个指令来工作真是好用得炸了，举例来说，我要检查/heany是否存在时，使用： test -e /heany 执行结果不会显示任何信息，但最后我们可以通过$?或&amp;&amp;及||来展现整个结果！例如我们在将上面的例子改写成这样： test -e /heany &amp;&amp; echo &quot;exist&quot; || echo &quot;No exist&quot; # output No exist ==&gt; 结果显示不存在 最终的结果可以告知我们是exist还是Not exist呢!那我知道-e是测试一个“东西”在不在,如果还想要测试一下该文件名是啥玩意儿时,还有哪些标志可以来判断的呢?呵呵!有下面这些东西喔! 关于某个文件名的”文件类型”判断，如test -e filename表示存在与否 测试的标志 代表意义 -e 该文件名是否存在 -f 该文件名是否存在且问文件 -d 该文件名是否存在且为目录 -b 该文件名是否存在且为一个block device设备 -c 该文件名是否存在且为一个character device设备 -S 该文件名是否存在且为一个Socket文件 -p 该文件名是否存在且为一个FIFO（pipe）文件 -L 该文件名是否存在且为一个链接文件 关于文件的权限侦测，如test -r filename表示可读否（但root权限常有例外） 两个文件之间的比较，如：test file1 -nt file2 测试的标志 代表意义 -nt (newer than)判断file1是否比file2新 -ot (older than)判断file1是否比file2旧 -ef 判断 file1 与 file2 是否为同一文件,可用在判断hard link的判定上。主要意义在判定,两个文件是否均指向同一个inode 关于两个整数之间的判定，例如：test n1 -eq n2 判定字串的数据 多重条件判定，例如：test -r filename -a -x filename 测试的标志 代表意义 -a 两个条件同时成立！例如 test -r file -a -x file,则file同时具有r与x权限时。才回传true -o (or)两状况任何一个成立!例如 test -r file -o -x file,则 file 具有 r 或 x 权限时,就可回传 true。 ! 反相状态,如test ! -x file,当file不具有x时,回传true 现在我们利用test来帮我们写几个简单的例子。首先，判断一下，让使用者输入一个文件名，我们判断： 这个文件是否存在，若不存在则给予一个”Filename does not exist”的信息，并中断程序; 若这个文件存在，则判断他是个文件或目录，结果输出“Filename is regular file”或”Filename is directory” 判断一下，执行者的身份对这个文件或目录所拥有的权限，并输出权限数据！ 123456789101112131415161718192021222324252627#!/bin/bash# Program:# User input a filename, program will check flowing:# 1. exist? 2. file/directory? 3. file permisions# History:# 2018/04/11 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATH# 1. 让使用者输入文件名,并且判断使用者是否真的有输入字串?echo -e \"Please input a filename, I will check the filename's type and permission. \\n\\n\"read -p \"Input a filename : \" filenametest -z $&#123;filename&#125; &amp;&amp; echo \"You MUST input a filename.\" &amp;&amp; exit 0# 2. 判断文件是否存在?若不存在则显示信息并结束脚本test ! -e $&#123;filename&#125; &amp;&amp; echo \"The filename '$&#123;filename&#125;' DO NOT exist\" &amp;&amp; exit 0# 3. 开始判断文件类型与属性test -f $&#123;filename&#125; &amp;&amp; filetype=\"regulare file\"test -d $&#123;filename&#125; &amp;&amp; filetype=\"directory\"test -r $&#123;filename&#125; &amp;&amp; perm=\"readable\"test -w $&#123;filename&#125; &amp;&amp; perm=\"$&#123;perm&#125; writable\"# 4. 开始输出信息!echo \"The filename: $&#123;filename&#125; is a $&#123;filetype&#125;\"echo \"And the permissions for you are : $&#123;perm&#125;\" 4.3.2、 利用判断符号[]除了我们很喜欢使用的test之外,其实,我们还可以利用判断符号[ ](就是中括号啦)来进行数据的判断呢! 举例来说,如果我想要知道${HOME}这个变量是否为空的,可以这样做: [ -z &quot;$HOME&quot;] ; echo $? 使用中括号必须要特别注意,因为中括号用在很多地方,包括万用字符与正则表达式等等,所以如果要在 bash 的语法当中使用中括号作为shell的判断式时,必须要注意中括号的两端需要有空白字符来分隔! 假设我空白键使用“□”符号来表示,那么,在这些地方你都需要有空白键: [ &quot;$HOME&quot; == &quot;$MAIL&quot;] [□&quot;$HOME&quot;□==□&quot;$MAIL&quot;□] 可是差很多的喔!另外,中括号的使用方法与test几乎一模一样啊~ 只是中括号比较常用在条件判断式if ..... then ..... fi的情况中就是了。好,那我们也使用中括号的判断来做一个小案例好了,案例设置如下: 1. 当执行一个程序的时候,这个程序会让使用者选择 Y 或 N , 2. 如果使用者输入Y或y时，就显示&quot;OK， continue&quot; 3. 如果使用者输入 n 或 N 时,就显示“ Oh, interrupt !” 4. 如果不是 Y/y/N/n 之内的其他字符,就显示“ I don&apos;t know what your choice is ” 利用中括号、&amp;&amp;与||来继续吧！12345678910111213#!/bin/bash# Program:# This program shows the user's choice.# History:# 2018/04/11 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHread -p \"Please input (Y/N): \" yn[ \"$&#123;yn&#125;\" == \"Y\" -o \"$&#123;yn&#125;\" == \"y\" ] &amp;&amp; echo \"OK, continue\" &amp;&amp; exit 0[ \"$&#123;yn&#125;\" == \"N\" -o \"$&#123;yn&#125;\" == \"n\" ] &amp;&amp; echo \"Oh, interrupt!\" &amp;&amp; exit 0echo \"I don't know what your choice is\" &amp;&amp; exit 0 这里使用-o来链接两个判断 4.3.3、 Shell Script的默认变量（$0,$1…）我们知道指令可以带有选项与参数,例如ls -la可以察看包含隐藏文件的所有属性与权限。那么shell script能不能在脚本文件名后面带有参数呢?很有趣喔!举例来说,如果你想要重新启动系统的网络,可以这样做: file /etc/init.d/network /etc/init.d/network restart # 重启 restart 是重新启动的意思,上面的指令可以“重新启动 /etc/init.d/network 这支程序”的意思!唔!那么如果你在 /etc/init.d/network 后面加上 stop 呢?没错!就可以直接关闭该服务了!这么神奇啊? 错啊!如果你要依据程序的执行给予一些变量去进行不同的任务时,本章一开始是使用 read 的功能!但 read 功能的问题是你得要手动由键盘输入一些判断式。如果通过指令后面接参数, 那么一个指令就能够处理完毕而不需要手动再次输入一些变量行为!这样下达指令会比较简单方便啦! script 是怎么达成这个功能的呢?其实 script 针对参数已经有设置好一些变量名称了!对应如下:12/path/to/scriptname opt1 opt2 opt3 opt4 $0 $1 $2 $3 $4 这样够清楚了吧?执行的脚本文件名为$0这个变量,第一个接的参数就是$1啊~ 所以,只要我们在 script 里面善用$1的话,就可以很简单的立即下达某些指令功能了!除了这些数字的变量之外, 我们还有一些较为特殊的变量可以在script内使用来调用这些参数喔! $# :代表后接的参数“个数”,以上表为例这里显示为“ 4 ”; $@ :代表“ &quot;$1&quot; &quot;$2&quot; &quot;$3&quot; &quot;$4&quot; ”之意,每个变量是独立的(用双引号括起来); $* :代表“ &quot;$1&lt;u&gt;c&lt;/u&gt;$2&lt;u&gt;c&lt;/u&gt;$3&lt;u&gt;c&lt;/u&gt;$4&quot; ”,&lt;br&gt; 其中 &lt;u&gt;c&lt;/u&gt; 为分隔字符,默认为空白键, 所以本例中代表“ &quot;$1 $2 $3 $4&quot; ”之意。 来练习：假设我们要执行一个可以携带参数的script，执行该脚本后屏幕会显示如下数据： 程序的文件名为何？ 共有几个参数？ 若参数的个数小于2则告知使用者参数数量太少 全部的参数内容为何？ 第一个参数为何？ 第二个参数为何？ 12345678910111213141516171819202122232425#!/bin/bash# Program:# Program shows the script name, parameters and so on....# History:# 2018/04/12 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHecho \"The script name is ===&gt; $0\"echo \"Total parameter number is ===&gt; $#\"[ \"$#\" -lt 2 ] &amp;&amp; echo \"The number of parameter is less than 2. Stop here.\"\\ &amp;&amp; exit 0echo \"Your whole parameter is ==&gt; '$@'\"echo \"The 1st parameter ===&gt; $1\"echo \"The 2nd parameter ===&gt; $2\"# 脚本执行sh how_paras.sh I am a boy# outputThe script name is ===&gt; how_paras.shTotal parameter number is ===&gt; 4Your whole parameter is ==&gt; 'I am a boy'The 1st parameter ===&gt; IThe 2nd parameter ===&gt; am shift：造成参数变量号码偏移 什么是偏移（shift）呢？我们用下面的范例来说明下，我们将how_paras.sh的内容稍作变化，用来显示每次偏移后参数的变化情况：1234567891011121314151617181920212223242526272829#!/bin/bash# Program:# Program shows the effect of shift function....# History:# 2018/04/12 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHecho \"Total parameter number is ===&gt; $#\"echo \"Your whole parameter is ===&gt; '$@'\"shift # 进行第一次\"一个变量的 shift\"echo \"Total parameter number is ===&gt; $#\"echo \"Your whole parameter is ===&gt; '$@'\"shift 3 #进行第二次\"三个变量的 shift\"echo \"Total parameter number is ===&gt; $#\"echo \"Your whole parameter is ===&gt; '$@'\"# execute scriptsh paras_shift.sh one two three four five six # outputTotal parameter number is ===&gt; 6Your whole parameter is ===&gt; 'one two three four five six'Total parameter number is ===&gt; 5Your whole parameter is ===&gt; 'two three four five six'Total parameter number is ===&gt; 2Your whole parameter is ===&gt; 'five six' 光看结果你就可以知道啦,那个shift会移动变量,而且shift后面可以接数字,代表拿掉最前面的几个参数的意思。 上面的执行结果中,第一次进行shift 后他的显示情况是one two three four five six,所以就剩下五个啦!第二次直接拿掉三个,就变成two three four five six啦!这样这个案例可以了解了吗?理解了shift的功能了吗? 4.4、 条件判断式4.4.1、 利用if .... then 单层、简单条件判断 如果你只有一个判断式要进行，那么我们可以简单的这样看： if [ 条件判断式 ]; then 当条件判断式成立时，可以进行命令工作内容 fi &lt;=== 将`if`反过来写，就成为`fi`，结束`if`的意思。 条件判断式有多个条件时，可以将多个条件写入一个中括号里面，也可以有多个中括号来隔开。括号之间，则用&amp;&amp;与||来隔开。举上面的ans_yn.sh例子来说明，修改里面的判断式： [ &quot;$yn&quot; == &quot;Y&quot; -o &quot;$yn&quot; == &quot;y&quot; ] 上式可替换为 [ &quot;$yn&quot; == &quot;Y&quot; ] || [ &quot;$yn&quot; == &quot;y&quot; ] 这样符合人们的习惯问题，我们都喜欢一个中括号仅有一个判别式。接下来，我们将ans_yn.sh这个脚本修改成为if ... then的样式： 12345678910111213141516171819202122#!/bin/bash# Program:# This program shows the user's choice.# History:# 2018/04/11 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHread -p \"Please input (Y/N): \" ynif [ \"$yn\" == \"Y\" ] || [ \"$yn\" == \"y\" ]; then echo \"OK, continue.\" exit 0fiif [ \"$yn\" == \"N\" ] || [ \"$yn\" == \"n\" ]; then echo \"Oh, interrupt!\" exit 0fiecho \"I don't know what your choice is\" &amp;&amp; exit 0 多重、复杂条件判断式 在同一个数据的判断中,如果该数据需要进行多种不同的判断时,应该怎么作?举例来说,上面的 ans_yn.sh 脚本中,我们只要进行一次 ${yn} 的判断就好 (仅进行一次 if ),不想要作多次 if 的判断。 此时你就得要知道下面的语法了: # 一个条件判断，分成功进行与失败进行(else) if [ 条件判断式一 ]; then 当条件判断式成立时，可以进行的指令工作内容： elif [ 条件判断式二 ]; then 当条件判断式二成立时，可以进行的指令工作内容 else 当条件判断式一和二都不成立时，可以进行的指令工作内容; fi 一般来说，如果你不希望使用者由键盘输入额外的数据时，可以使用上面提到的参数功能（$1），让使用者在下达命令时就将参数带进去！现在我们想让使用者输入hello这个关键字时，利用参数的方法可以这样依序设计： 1. 判断`$1`是否为hello，如果是的话，就显示“hello，how are you？” 2. 如果没有加任何参数,就提示使用者必须要使用的参数下达法; 3. 而如果加入的参数不是 hello ,就提醒使用者仅能使用 hello 为参数。 之前我们已经学会了grep这个好用的玩意儿，那么多学一个叫做netstat的命令，这个指令可以查询到目前主机有打开的网络服务端口（service ports）,我们可以利用netstat -tuln来取得目前主机有启动的服务，而且取得的信息有点像这样：12345678910netstat -tulnActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:5355 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:1080 0.0.0.0:* LISTEN udp 0 0 0.0.0.0:5353 0.0.0.0:* udp 0 0 0.0.0.0:5353 0.0.0.0:* #封包格式 本地IP：端口 远程IP：端口 是否监听 上面的重点是“Local Address (本地主机的IP与端口对应)”那个字段,他代表的是本机所启动的网络服务! IP的部分说明的是该服务位于那个接口上,若为 127.0.0.1 则是仅针对本机开放,若是 0.0.0.0 或 ::: 则代表对整个Internet开放。 每个端口 (port) 都有其特定的网络服务,几个常见的 port 与相关网络服务的关系是: 80：www 22:ssh 21:ftp 25:mail 111:RPC(远端程序调用) 631:CUPS（打印服务功能） 假设我的主机有兴趣要侦测的是比较常见的 port 21, 22, 25及 80 时,那我如何通过netstat去侦测我的主机是否有打开这四个主要的网络服务端口呢?由于每个服务的关键字都是接在冒号:后面, 所以可以借由提取类似:80来侦测的!那我就可以简单的这样去写这个程序喔: 4.4.2、 利用case ... esac语法如下： case $变量名称 in &lt;=== 关键字为case。还有变量前有$ &quot;第一个变量内容&quot;) &lt;=== 每个变量内容建议用双引号括起来，关键字则为小括号） 程序段 ;; &lt;=== 每个类别结尾i使用两个连续的分号来处理 &quot;第二个变量内容&quot;） 程序段 ;; *) &lt;=== 最后一个变量内容都会用*来代表所有其他值 不包括第一个变量内容与第二个变量内容的其他程序执行段 exit 1 ;; esac &lt;=== 最终的case结尾，“反过来写” 这么说或许你的感受性还不高,好,我们直接写个程序来玩玩:让使用者能够输入 one, two,three , 并且将使用者的变量显示到屏幕上,如果不是 one, two, three 时,就告知使用者仅有这三种选择。1234567891011121314151617181920212223242526#!/bin/bash# Program:# This script only accepts the flowing parameter: one, two or three.# History:# 2018/04/12 heany Fist releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHecho \"This program will print your selection !\"# read -p \"Input your choice: \" choice # 暂时取消,可以替换!# case $&#123;choice&#125; in # 暂时取消,可以替换!case $&#123;1&#125; in # 现在使用,可以用上面两行替换! \"one\") echo \"Your choice is ONE\" ;; \"two\") echo \"Your choice is TWO\" ;; \"three\") echo \"Your choice is THREE\" ;; *) echo \"Usage $&#123;0&#125; &#123;one|two|three&#125;\" ;;esac 4.4.3、 利用function功能什么是“函数 (function)”功能啊?简单的说,其实, 函数可以在 shell script 当中做出一个类似自订执行指令的东西,最大的功能是, 可以简化我们很多的程序码~举例来说,上面的show123.sh 当中,每个输入结果 one, two, three 其实输出的内容都一样啊~那么我就可以使用 function 来简化了! function 的语法是这样的: function fname(){ 程序段 } 由于shell script的执行方式是由上而下,由左而右, 因此在shell script当中的function的设置一定要在程序的最前面, 这样才能够在执行时被找到可用的程序段喔(这一点与传统程序语言差异相当大!初次接触的朋友要小心!)! 好~我们将上面的脚本改写一下,自定义一个名为printit的函数来使用: 123456789101112131415161718192021222324# History:# 2018/04/12 heany First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHfunction printit()&#123; echo -n \"Your choice is \" # 加上 -n 可以不换行继续在同一行显示&#125;echo \"This program will print your selection !\"case $&#123;1&#125; in \"one\") printit; echo $1 | tr 'a-z' 'A-Z' # 将参数做大小写转换! ;; \"two\") printit; echo $1 | tr 'a-z' 'A-Z' ;; \"three\") printit; echo $1 | tr 'a-z' 'A-Z' ;; *) echo \"Usage $&#123;0&#125; &#123;one|two|three&#125;\"\"show123_function.sh\" 29L, 591C written 11,53-55 50% 4.5、 循环（loop）除了 if…then…fi 这种条件判断式之外,循环可能是程序当中最重要的一环了~ 循环可以不断的执行某个程序段落,直到使用者设置的条件达成为止。所以,重点是那个“条件的达成”是什么。除了这种依据判断式达成与否的不定循环之外,循环形态,可称为固定循环的形态呢!下面我们就来谈一谈: 4.5.1、 while do done,until do done（不定循环）一般来说，不定循环最常见的就是下面这两种状态了： while [ condition ] &lt;=== 中括号内的状态就是判别式 do 代码段 done while的中文是“当…时”，所以，这种方式说的是当condition条件成立时，就进行循环，直到condition的条件不成立才停止的意思。还有另为一种不定循环的方式： until [ condition ] do 代码段 done 这种方式恰恰与 while 相反,它说的是“当 condition 条件成立时,就终止循环, 否则就持续进行循环的程序段。”是否刚好相反啊~我们以 while 来做个简单的练习好了。 假设我要让使用者输入 yes 或者是 YES 才结束程序的执行,否则就一直进行告知使用者输入字串。12345678910111213141516171819#!/bin/bash# Program:# Repeat question until user input correct answer.# History:# 2018/04/12 leagle First release# whilewhile [ \"$yn\" != \"yes\" -a \"$yn\" != \"YES\" ] do read -p \"Please input yes/YES to stop this program: \" yndone echo \"OK! you input the correct answer.\"# untiluntil [ \"$yn\" == \"yes\" -o \"$yn\" == \"YES\" ] do read -p \"Please input yes/YES to stop this program: \" yndoneecho \"OK! you input the correct answer.\" 4.5.2、 for ... do ...done(固定循环)for循环的语法如下所示： for var in con1 con2 con3 ... do code done 简单练习。12345678910#!/bin/bash# Program:# Using for ... loop to print 3 animal.# History:# 2018/04/12 leagle First releasefor animal in dog cat elephantdo echo \"There are $&#123;animal&#125;s ....\"done 让我们想像另外一种状况,由于系统上面的各种帐号都是写在 /etc/passwd 内的第一个字段,你能不能通过管线命令的 cut 捉出单纯的帐号名称后,以id分别检查使用者的识别码与特殊参数呢?由于不同的 Linux 系统上面的帐号都不一样!此时实际去捉/etc/passwd并使用循环处理,就是一个可行的方案了!程序可以如下:1234567891011#!/bin/bash# Program:# Use id, finger command to check system account's information.# History:# 2018/04/12 leagle First releaseusers=$(cut -d ':' -f1 ./pd.txt) # 提获帐号名称for username in $&#123;users&#125; # 开始循环进行!do id $&#123;username&#125;done 执行上面的脚本后,你的系统帐号就会被捉出来检查啦!这个动作还可以用在每个帐号的删除、重整上面呢! 换个角度来看,如果我现在需要一连串的数字来进行循环呢?举例来说,我想要利用 ping 这个可以判断网络状态的指令, 来进行网络状态的实际侦测时,我想要侦测的网域是本机所在的 192.168.1.1~192.168.1.100,由于有 100 台主机, 总不会要我在 for 后面输入 1 到 100 吧?此时你可以这样做喔!123456789101112131415161718192021#!/bin/bash# Program:# Use ping command to check the network's PC state.# History:# 2018/04/12 leagle First releasenetwork=\"192.168.1\" # 先定义一个网域的前面部分!for sitenu in $(seq 1 100) # seq 为 sequence(连续) 的缩写之意do # 下面的程序在取得 ping 的回传值是正确的还是失败的! ping -c 1 -w 1 $&#123;network&#125;.$&#123;sitenu&#125; &amp;&gt; /dev/null &amp;&amp; result=0 || result=1 # 开始显示结果是正确的启动 (UP) 还是错误的没有连通 (DOWN) if [ \"$&#123;result&#125;\" == 0 ]; then echo \"Server $&#123;network&#125;.$&#123;sitenu&#125; is UP.\" else echo \"Server $&#123;network&#125;.$&#123;sitenu&#125; is DOWN.\" fidone 注意那个$(seq 1 100)那个位置，那个是连续产生1-100个数。 最后,让我们来玩判断式加上循环的功能!我想要让使用者输入某个目录文件名,然后我找出某目录内的文件名的权限,该如何是好?可以这样做啦~123456789101112131415161718192021222324#!/bin/bash# Program:# Use input dir name, I find the permission of files.# History:# 2018/04/12 leagle First release# 1. 先看看这个目录是否存在啊?read -p \"Please input a directory: \" dirif [ \"$&#123;dir&#125;\" == \"\" -o ! -d \"$&#123;dir&#125;\" ]; then echo \"The $&#123;dir&#125; is NOT exist in your system.\" exit 1fi# 2. 开始测试文件~filelist=$(ls $&#123;dir&#125;) # 列出所有在该目录下的文件名称for filename in $&#123;filelist&#125;do perm=\"\" test -r \"$&#123;dir&#125;/$&#123;filename&#125;\" &amp;&amp; perm=\"$&#123;perm&#125; readable\" test -w \"$&#123;dir&#125;/$&#123;filename&#125;\" &amp;&amp; perm=\"$&#123;perm&#125; writable\" test -x \"$&#123;dir&#125;/$&#123;filename&#125;\" &amp;&amp; perm=\"$&#123;perm&#125; executable\" echo \"The file $&#123;dir&#125;/$&#123;filename&#125;'s permission is $&#123;perm&#125; \"done 4.5.3 for ... do ...done的数值处理除了上述的方法之外，for循环还有另外一种写法，类似于C语言中的for循环！语法如下所示： for ((初始值;限制值;执行步长)) do code done 这种语法很适合用于数值方式的运算当中，值得注意的是,在“执行步阶”的设置上,如果每次增加 1 ,则可以使用类似i++的方式,亦即是 i 每次循环都会增加一的意思。好,我们以这种方式来进行 1 累加到使用者输入的循环吧!1234567891011121314#!/bin/bash# Program:# Try to calculate \"1+2+3+...+$&#123;yourinput&#125;\".# History:# 2018/04/12 leagle First releaseread -p \"Please input a number, I will count for 1+2+...+your_input: \" nus=0for (( i=1; i&lt;=$nu; i=i+1))do s=$(($s+$i))done echo \"The result of '1+2+...+$nu' is ===&gt; $s\" 4.6、 Shell Script的追踪与Debugscripts在执行之前,最怕的就是出现语法错误的问题了!那么我们如何debug 呢?有没有办法不需要通过直接执行该scripts就可以来判断是否有问题呢?呵呵!当然是有的!我们就直接以 bash 的相关参数来进行判断吧! # 语法如下： sh [-nvx] scripts.sh # paras: -n ：不要执行script，仅查询语法的问题; -v ：在执行script之前，先将scripts的内容输出到屏幕上; -x ：将使用到的script内容显示到屏幕上，这是很有用的参数！ 一些范例测试：12345678910bash -x ans_yn-2.sh + PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:/home/heany/bin+ export PATH+ read -p 'Please input (Y/N): ' ynPlease input (Y/N): y+ '[' y == Y ']'+ '[' y == y ']'+ echo 'OK, continue.'OK, continue.+ exit 0 五、 结束语&emsp;&emsp;这个系列终于结束了，码字调bug很辛苦哇～但是从中学到了很多有用的知识，解决了之前很多搞不清楚的问题，总之很棒啦！本文转载自鸟哥的Linux私房菜：基础学习篇 第四版。总之这本书非常适合入门学习～强烈推荐！！！","tags":[{"name":"Shell linux","slug":"Shell-linux","permalink":"http://heany.github.io/tags/Shell-linux/"}]},{"title":"hexo常用命令","date":"2017-10-28T06:18:08.000Z","path":"2017/10/28/hexo常用命令/","text":"现在来介绍常用的Hexo 命令 npm install hexo -g #安装Hexo npm update hexo -g #升级 hexo init #初始化博客 命令简写 hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; #新建文章 hexo g == hexo generate #生成 hexo s == hexo server #启动服务预览 hexo d == hexo deploy #部署 hexo server #Hexo会监视文件变动并自动更新，无须重启服务器 hexo server -s #静态模式 hexo server -p 5000 #更改端口 hexo server -i 192.168.1.1 #自定义 IP hexo clean #清除缓存，若是网页正常情况下可以忽略这条命令 博客搭建好以后，发表文章步骤及命令 step1：切换到工作目录，执行hexo n &quot;文章的title&quot; , 然后看到source/_posts会出现刚刚新建的文章的title.md step2：进去编辑刚刚生成的md文件 step3：执行命令hexo clean step4: 执行命令hexo g step5: 执行命令hexo d等待一会就能看到博客主页出现了刚刚编写的文章 卸载hexo： npm uninstall hexo-cli -g #3.0.0版本执行 npm uninstall hexo -g # 之前版本执行","tags":[{"name":"hexo常用命令","slug":"hexo常用命令","permalink":"http://heany.github.io/tags/hexo常用命令/"}]},{"title":"如何批量删除微博","date":"2017-10-28T05:44:52.000Z","path":"2017/10/28/如何批量删除微博/","text":"最近想删掉自己的所有微博，可是微博官方并没有相关的功能，网上也有一些工具能够实现，但是都收费。所以我搜了一些比较方便的方法，来实现删除所有微博。 step1: 打开Chrome，新开一个标签页，进入我的微博“我的首页” step2：打开Chrome的开发者工具（按F12） step3: 点击console，打开控制台，将下面的代码贴进去，回车运行 将下面的代码贴近进console里1234567891011121314151617181920212223242526272829// ==UserScript==// @name Weibored.js// @namespace http://vito.sdf.org// @version 0.2.0// @description 删除所有微博// @match http://weibo.com/p/*// @grant none// ==/UserScript=='use strict';var s = document.createElement('script');s.setAttribute( 'src', 'https://lib.sinaapp.com/js/jquery/2.0.3/jquery-2.0.3.min.js');s.onload = function() &#123; setInterval(function() &#123; if (!$('a[action-type=\"feed_list_delete\"]')) &#123; $('a.next').click(); &#125; else &#123; $('a[action-type=\"feed_list_delete\"]')[0].click(); $('a[action-type=\"ok\"]')[0].click(); &#125; // scroll bottom let auto load $('html, body').animate(&#123; scrollTop: $(document).height() &#125;, 'slow'); &#125;, 800);&#125;;document.head.appendChild(s); 回车运行 此时能够看到微博主页开始滚动","tags":[]},{"title":"sublime插件：Markdown","date":"2017-10-15T13:30:12.000Z","path":"2017/10/15/sublime插件：Markdown/","text":"1.MarkDown Editing：支持Markdown语法高亮；支持Github Favored Markdown语法；自带3个主题 2.MarkdownPreview：按CTRL + B生成网页HTML；在最前面添加[TOC]自动生成目录；3.Markdown Extended + Extends Mononokai：不错的Markdown主题，支持对多种语言的高亮4.OmniMarkupPreviwer：实时在浏览器中预，而MarkdownPreview是需要手动生成的和F5的。览如果双屏的话，应该具有不错的体验。快捷键如下： Ctrl+Alt+o: Preview Markup in Browser Ctrl+Alt+x: Export Markup as HTML Ctrl+Alt+c: Copy Markup as HTML. 5.TableEditor：Markdown中的表格书写体验真心不咋样，所有有人为这个开发了一个插件，具有较好的自适应性，会自动对齐，强迫症患者喜欢。首先需要用ctrl+shift+p打开这个功能（Table Editor: Enable for current syntax or Table Editor: Enable for current view or “Table Editor: Set table syntax … for current view”），然后就可以狂用tab来自动完成了 6.Markdown TOC：编辑MD文件的时候可以查看自动生成，并且可以控制生产目录的层次，不过不会自动跳转。编辑的时候可以看看，如果需要生成的HTML具有超链接跳转的功能，还是用MarkdownPreview吧。","tags":[{"name":"sublime插件","slug":"sublime插件","permalink":"http://heany.github.io/tags/sublime插件/"},{"name":"Markdown","slug":"Markdown","permalink":"http://heany.github.io/tags/Markdown/"}]},{"title":"如何上传本地文件到Github上","date":"2017-10-15T13:14:57.000Z","path":"2017/10/15/如何上传本地文件到Github上/","text":"最近做了一些工作，想将他们上传到GitHub上去保存，我习惯于本地编辑，完成后再一起上传到仓库中，长时间不使用很容易忘，总结网上的一些教程，写作博客里方便自己偶尔查阅。 本文参考以下博文两种方法上传本地文件到github 想知道详细的步骤可以点击前往学习，这里我只简单记录一些常用的命令。 1.GitHub在线上传文件夹1.1点击Upload files1.2 直接拖拽2.通过git工具上传本地文件夹2.1下载git 由于部分原因，国内直接官网下载git非常慢，需要FQ。这里提供一个国内的下载站，方便网友下载。 点击前往 2.2 绑定用户打开git-bash.exe（直接在桌面上点击右键，或者点击开始按钮找到Git Bash） 在打开的GIt Bash中输入以下命令（用户和邮箱为你github注册的账号和邮箱） $ git config --global user.name &quot;##&quot; $ git config --global user.email &quot;h##@**.com&quot; 2.3 设置SSH Key2.3.1 生成ssh key2.3.2 为GitHub账号配置ssh key2.4 上传本地项目到GitHub 这里只贴一些命令 1234567891011121314# 初始化git init# 将所有文件添加到仓库中git add .# 提交git commit -m \"注释\"# 关联GitHub仓库git remote add origin https://github.com/**/##.git# 上传本地代码及文件git push -u origin master","tags":[]},{"title":"在sublimeText3中安装pylinter方法","date":"2017-10-15T06:03:54.000Z","path":"2017/10/15/在sublimeText3中安装pylinter方法/","text":"1.下载pylint安装包官网下载，解压即可&lt;https://pypi.python.org/pypi/pylint&gt; 2.安装 astroidpip install astroid 或者 &nbsp;&nbsp;&nbsp;&nbsp;打开https://bitbucket.org/logilab/astroid,下载压缩包，解压 cd到下载好的文件夹内，然后使用 python setup.py install 3.安装isort,用如下命令pip install isort 4.安装pylint &nbsp;&nbsp;&nbsp;&nbsp;cmd切换到刚才下载解压pylint文件夹内，输入命令： python setup.py install 5.安装pylint插件 在sublime中ctrl+shift+P 然后输入install 然后输入pylinter，回车即可！","tags":[{"name":"sublime 插件","slug":"sublime-插件","permalink":"http://heany.github.io/tags/sublime-插件/"}]},{"title":"python3.x与MySQL的安装与链接","date":"2017-10-15T06:02:45.000Z","path":"2017/10/15/python3-x与MySQL的安装与链接/","text":"需要用到python来进行mysql数据库处理，遇到了一些问题，比如python如何连接MySQL进行数据库相关操作等，接下来我将一些python3中一些MySQL处理的一些操作写在下面。 python2.x版本连接MySQL数据库需要安装MySQL-python库，打开终端输入命令： pip install MySQL-python 或者 pip install MySQLdb python3.x版本链接MySQL数据库需要安装PyMySQL库，打开终端输入命令： pip install PyMySQL 还可以通过Pycharm软件里面的库管理来添加pymysql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import pymysql # 连接数据库 connect = pymysql.Connect( host='localhost', port=3306, user='root', passwd='1234', db='save', charset='utf8' ) # 获取游标 cursor = connect.cursor() # 插入数据 sql = \"INSERT INTO money (name, account, saving) VALUES ( '%s', '%s', %.2f )\" data = ('雷军', '13512345678', 10000) cursor.execute(sql % data) connect.commit() print('成功插入', cursor.rowcount, '条数据') # 修改数据 sql = \"UPDATE money SET saving = %.2f WHERE account = '%s' \" data = (8888, '13512345678') cursor.execute(sql % data) connect.commit() print('成功修改', cursor.rowcount, '条数据') # 查询数据 sql = \"SELECT name,saving FROM money WHERE account = '%s' \" data = ('13512345678',) cursor.execute(sql % data) for row in cursor.fetchall(): print(\"Name:%s\\tSaving:%.2f\" % row) print('共查找出', cursor.rowcount, '条数据') # 删除数据 sql = \"DELETE FROM money WHERE account = '%s' LIMIT %d\" data = ('13512345678', 1) cursor.execute(sql % data) connect.commit() print('成功删除', cursor.rowcount, '条数据') # 事务处理 sql_1 = \"UPDATE money SET saving = saving + 1000 WHERE account = '18012345678' \" sql_2 = \"UPDATE money SET expend = expend + 1000 WHERE account = '18012345678' \" sql_3 = \"UPDATE money SET income = income + 2000 WHERE account = '18012345678' \" try: cursor.execute(sql_1) # 储蓄增加1000 cursor.execute(sql_2) # 支出增加1000 cursor.execute(sql_3) # 收入增加2000 except Exception as e: connect.rollback() # 事务回滚 print('事务处理失败', e) else: connect.commit() # 事务提交 print('事务处理成功', cursor.rowcount) # 关闭连接 cursor.close() connect.close()","tags":[{"name":"python学习笔记","slug":"python学习笔记","permalink":"http://heany.github.io/tags/python学习笔记/"}]},{"title":"xpath学习笔记","date":"2017-10-15T06:00:09.000Z","path":"2017/10/15/xpath学习笔记/","text":"XPath是一个非常好用的解析方法，同时也作为爬虫学习的基础，在后面的 selenium 以及 scrapy 框架中都会涉及到这部分知识，需要把它的语法掌握清楚，为后面的深入研究做好铺垫。 学习参考转自Python爬虫利器三之Xpath语法与lxml库的用法 xpath语法参考w3school w3school 选取节点 XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。 下面列出了最有用的路径表达式 表达式 描述 nodename 选取此节点的所有子节点 / 从根结点选取 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 实例在下面的表格中，我们已经列出了一些路径表达式以及表达式的结果： 实例Demo12345678&lt;bookstore&gt;&lt;book&gt; &lt;title&gt;Harry Potter&lt;/title&gt; &lt;author&gt;J K. Rowling&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 实例测试 在下面的表格中，我们已列出了一些路径表达式以及表达式的结果： 谓语谓语用来查找某个特定的节点或者包含某个指定的值的节点。 谓语被嵌在方括号中。 实例 在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 选取未知节点XPath 通配符可用来选取未知的 XML 元素。 通配符 描述 * 匹配任何节点元素 @* 匹配任何属性节点 node() 匹配任何类型的节点 通配符 描述 * 匹配任何节点元素 @* 匹配任何属性节点 node() 匹配任何类型的节点 实例 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 /bookstore/* 选取bookstore元素的所有的子元素 //* 选取文档中的所有元素 //title[@*] 选取所有带有属性的title元素 选取若干路径 通过在路径表达式中使用“|”运算符，可以选择若干个路径 实例 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 //book/title \\ //book/price 选取 book 元素的所有 title 和 price 元素。 //title \\ //price 选取文档中的所有 title 和 price 元素。 /bookstore/book/title \\ //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 Xpath实例测试 测试Demo:123456789&lt;div&gt; &lt;ul&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link1.html\"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link2.html\"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-inactive\"&gt;&lt;a href=\"link3.html\"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-1\"&gt;&lt;a href=\"link4.html\"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=\"item-0\"&gt;&lt;a href=\"link5.html\"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt; 获取所有的&lt;l\\&gt;标签1response.xpath('//li') 获取 &lt;li&gt; 标签的所有 class12345response.xpath('//li/@class')运行结果['item-0', 'item-1', 'item-inactive', 'item-1', 'item-0'] 获取 &lt;li&gt; 标签下 href 为 link1.html 的 标签1234response.xpath('//li/a[@href=\"link1.html\"]')运行结果[&lt;Element a at 0x10ffaae18&gt;] 获取 &lt;li&gt; 标签下的所有 &lt;span&gt; 标签 注意这种写法是不对的 response.xpath(&apos;//li/span&apos;) 因为/是用来获取子元素的，而并不是&lt;li&gt;的子元素，所以要用双斜杠，如下所示： response.xpath(&apos;//li//span&apos;) 运行结果： [&lt;Element span at 0x10d698e18&gt;] 获取&lt;li&gt;标签下的所有class，不包括&lt;li&gt; result = response.xpath(&apos;&apos;//li/a/@class&apos;) print result 运行结果： [&apos;blod&apos;] 获取最后一个&lt;li&gt;的&lt;a&gt;的href1response.xpath('//li[last()]/a/@href') 运行结果： [&apos;link5.html&apos;] 获取倒数第二个元素的内容12result = response.xpath('//li[last()-1]/a')print result[0].text 运行结果： fourth item 获取class为bold的标签名12result = response.xpath('//*[@class=\"bold\"]')print result[0].tag 运行结果： span 通过以上实例的练习，能够对XPath的基本用法有了基本的了解，也可以通过text()方法获取元素内容。","tags":[{"name":"python学习笔记","slug":"python学习笔记","permalink":"http://heany.github.io/tags/python学习笔记/"}]},{"title":"贝叶斯分类器实现","date":"2017-10-14T11:03:31.000Z","path":"2017/10/14/贝叶斯分类器实现/","text":"1、贝叶斯分类实现（采用python3.6 数据集为arff文件格式）数据挖掘课老师留的一个实现贝叶斯分类器的练习，数据源为arff格式的文件~下面是使用python语言编写的一个简单的实现案例。 1.1、 arff格式文件介绍参考链接 arff文件是Weka默认的储存数据集文件。每个arff文件对应一个二维表格。表格的各行是数据集的各实例，各列是数据集的各个属性。推荐使用UltraEdit这样的字符编辑软件或者Sublime Text这样的文本编译器查看arff文件的内容。 weather.arff 文件内容如下所示： 识别ARFF文件的重要依据是分行，因此不能在这种文件里随意的断行。空行（或全是空格的行）将被忽略。以“%”开始的行是注释，WEKA将忽略这些行。如果你看到的“weather.arff”文件多了或少了些“%”开始的行，是没有影响的。除去注释后，整个ARFF文件可以分为两个部分。第一部分给出了头信息（Head information），包括了对关系的声明和对属性的声明。第二部分给出了数据信息（Data information），即数据集中给出的数据。从“@data”标记开始，后面的就是数据信息了。虽然Weka也支持其他一些格式的文件，但是ARFF格式是支持的最好的。因此有必要在数据处理之前把数据集的格式转换成ARFF。 1.2、 arff格式文件读取1234567891011121314151617import re import sys def readArff(fileName): arffFile = open(fileName,'r') data = [] for line in arffFile.readlines(): if not (line.startswith('@')): if not (line.startswith('%')): if line !='\\n': L=line.strip('\\n') k=L.split(',') data.append(k) print(k) print(data) if __name__ =='__main__': fileName=r'C:\\Users\\Administrator\\Desktop\\exepirenment\\classifill\\data\\weather.arff' readArff(fileName) 输出结果如下所示：123456789101112131415['sunny', '85', '85', 'FALSE', 'no']['sunny', '80', '90', 'TRUE', 'no']['overcast', '83', '86', 'FALSE', 'yes']['rainy', '70', '96', 'FALSE', 'yes']['rainy', '68', '80', 'FALSE', 'yes']['rainy', '65', '70', 'TRUE', 'no']['overcast', '64', '65', 'TRUE', 'yes']['sunny', '72', '95', 'FALSE', 'no']['sunny', '69', '70', 'FALSE', 'yes']['rainy', '75', '80', 'FALSE', 'yes']['sunny', '75', '70', 'TRUE', 'yes']['overcast', '72', '90', 'TRUE', 'yes']['overcast', '81', '75', 'FALSE', 'yes']['rainy', '71', '91', 'TRUE', 'no'][['sunny', '85', '85', 'FALSE', 'no'], ['sunny', '80', '90', 'TRUE', 'no'], ['overcast', '83', '86', 'FALSE', 'yes'], ['rainy', '70', '96', 'FALSE', 'yes'], ['rainy', '68', '80', 'FALSE', 'yes'], ['rainy', '65', '70', 'TRUE', 'no'], ['overcast', '64', '65', 'TRUE', 'yes'], ['sunny', '72', '95', 'FALSE', 'no'], ['sunny', '69', '70', 'FALSE', 'yes'], ['rainy', '75', '80', 'FALSE', 'yes'], ['sunny', '75', '70', 'TRUE', 'yes'], ['overcast', '72', '90', 'TRUE', 'yes'], ['overcast', '81', '75', 'FALSE', 'yes'], ['rainy', '71', '91', 'TRUE', 'no']] 1.3、 实现贝叶斯分类器参考链接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import re import sys import bisect data =[] #全局变量 def readArff(fileName): arffFile = open(fileName,'r') global data for line in arffFile.readlines(): if not (line.startswith('@')): if not (line.startswith('%')): if line !='\\n': L=line.strip('\\n') k=L.split(',') data.append(k) def bayesion(testData): class1=[] class2=[] global data for item in data: if item[len(item)-1] == 'yes': class1.append(item) else: class2.append(item) class1Probability = len(class1) /len(data) class2Probability = len(class2) /len(data) for i in range(len(testData)): count = 0 for elem in class1: if testData[i]==elem[i]: count +=1 #统计个数 class1Probability *= count/len(class1) #累计乘法 求总概率 count = 0 for elem in class2: if testData[i]==elem[i]: count +=1 class2Probability *=count/len(class2) if class1Probability &gt;class2Probability: #比较，进而分类 print(\"The result is : Yes\") else: print(\"The result if : No\") #数据预处理，将data数据分箱，data数据为 list[list1,list2...]类型 def dataPreprocessing1(data): breakpoint1 =[70,80] breakpoint2=[80,90] newValue1='LMH' for item in data: i = bisect.bisect(breakpoint1,int(item[1])) #int(),str()等类型需要转换 item[1]=str(newValue1[i]) j = bisect.bisect(breakpoint2,int(item[2])) item[2]=str(newValue1[j]) #数据预处理，分箱，针对单个list数据，data为list['..','..']类型 def dataPreprocessing2(data): breakpoint1 =[70,80] breakpoint2=[80,90] newValue1='LMH' i = bisect.bisect(breakpoint1,int(data[1])) data[1]=str(newValue1[i]) j = bisect.bisect(breakpoint2,int(data[2])) data[2]=str(newValue1[j]) if __name__ =='__main__': fileName=r'C:\\Users\\Administrator\\Desktop\\exepirenment\\classifill\\data\\weather.arff' readArff(fileName) dataPreprocessing1(data) testData =['overcast','72','80','TRUE'] dataPreprocessing2(testData) bayesion(testData)","tags":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"http://heany.github.io/tags/数据挖掘/"},{"name":"python3","slug":"python3","permalink":"http://heany.github.io/tags/python3/"},{"name":"贝叶斯分类器","slug":"贝叶斯分类器","permalink":"http://heany.github.io/tags/贝叶斯分类器/"}]},{"title":"我的博客搭建流程","date":"2017-10-08T12:42:48.000Z","path":"2017/10/08/我的博客搭建流程/","text":"一、前言Hexo部署到GitHub上的文件，是.md（你的博文）转化之后的.html（静态网页）。因此，当你重装电脑或者想在不同电脑上修改博客时，就不可能了（除非你自己写html）。其实，Hexo生成的网站文件中有.gitignore文件，因此它的本意也是想我们将Hexo生成的网站文件存放到GitHub上进行管理的（而不是用U盘或者云备份啦）。这样，不仅解决了上述的问题，还可以通过git的版本控制追踪你的博文的修改过程，是极赞的。 但是，如果每一个GitHub Pages都需要创建一个额外的仓库来存放Hexo网站文件，我感觉很麻烦（10个项目需要20个仓库…）。 所以，我利用了分支！！！ 简单地说，每个想建立GitHub Pages的仓库，起码有两个分支，一个用来存放Hexo网站的文件，一个用来发布网站。 下面将我的博客搭建流程简单讲一下 二、 我的博客搭建流程 创建仓库，yourname.github.io; 创建两个分支: master（这个分支主要用来展示静态网页，给用户看的User Pages）和hexo（这个分支主要是用来备份Projects Pages）; 设置hexo为默认分支; 使用git clone git@github.com:yourname/yourname.github.io拷贝仓库，拷贝完成后，本地会有一个yourname.github.com文件夹; 由于hexo init dir命令不能在非空的dir下执行成功,所以先找一个文件夹(例如blog)建立一个博客，也就是依次执行下面命令： 123hexo init blognpm installnpm install hexo-deployer-git --save 将上面blog文件夹下所有的文件全部拷贝到刚刚clone下来的yourname.github.io文件夹里面。 修改yourname.github.io/_config.yml中的deploy参数，分支应该为master。 1234deploy: type: git repo: git@github.com:yourname/yourname.github.io.git branch: master 依次执行下面命令,将网站相关的文件备份到github上。 123git add .git commit -m \"your comments\"git push origin hexo 然后执行hexo相关，命令生成网站，并部署到Github上： 123hexo clean #清楚缓存，若是网页正常情况下可以忽略这条命令hexo g # 生成 是hexo generatehexo d # 部署 是hexo deploy 如此操作，在Github上的heany.github.io仓库就有两个分支，一个hexo分支用来存放网站的原始文件，一个master分支用来存放生成的静态网页～效率超高，不用再新建一个repository去备份网站文件了！是不是很好～ 三、 博客日常管理流程3.1、 日常修改在本地对博客进行修改（添加新博文、修改样式等等）后，通过下面的流程进行管理： 依次执行git add .、git commit -m “…”、git push origin hexo指令将改动推送到GitHub（此时当前分支应为hexo）； 然后才执行hexo generate -d发布网站到master分支上。 虽然两个过程顺序调转一般不会有问题，不过逻辑上这样的顺序是绝对没问题的（例如突然死机要重装了，悲催….的情况，调转顺序就有问题了）。 3.2、 本地资料丢失当重装电脑之后，或者想在其他电脑上修改博客，可以使用下列步骤： 使用git clone git@github.com:yourname/yourname.github.io.git拷贝仓库（默认分支为hexo）； 在本地新拷贝的yourname.github.io文件夹下通过Git bash(windows下使用。若是linux则在bash里面直接执行)依次执行下列指令：npm install hexo、npm install、npm install hexo-deployer-git（记住不需要hexo init这条指令）。 四、 结束语Github Pages是支持域名绑定的，可以去Godday上买一个比较便宜的域名，然后在国内的DNSPod上去解析你的域名,。具体参考这篇文章摸我前往。 到这里结束了，好累啊～～～","tags":[{"name":"hexo Github-Pages Blog","slug":"hexo-Github-Pages-Blog","permalink":"http://heany.github.io/tags/hexo-Github-Pages-Blog/"}]},{"title":"Hello World","date":"2017-10-08T06:42:48.000Z","path":"2017/10/08/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[{"name":"hexo Blog","slug":"hexo-Blog","permalink":"http://heany.github.io/tags/hexo-Blog/"}]}]